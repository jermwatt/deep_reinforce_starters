{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learner for cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Q-Learning algo written in Keras for the cartpole\n",
    "\n",
    "- Batch version used here, Q function updated after each episode of simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os \n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "# import custom reinforcement library\n",
    "import reinforcement_library as reinlib\n",
    "\n",
    "# import cartpole + pytorch online q-learner\n",
    "learner = reinlib.deep_Q_Learning.qfitted_deepQlearning_keras\n",
    "plotter = reinlib.deep_Q_Learning.history_plotter\n",
    "\n",
    "# load in autoreload so any changes made to backend files mirrored in notebook\n",
    "# without the need to restart kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-Learner Algo can be loaded in from backend file by activating the command\n",
    "\n",
    "```learner.QLearner??```\n",
    "\n",
    "in a code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning setup interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# feed in current directory name and savename = experiment name\n",
    "dirname = os.getcwd()\n",
    "savename = 'batch_cartpole_experiment_1'\n",
    "\n",
    "# gymname = 'LunarLander-v2'\n",
    "gymname = 'CartPole-v1'\n",
    "\n",
    "# initialize Q Learn process\n",
    "num_episodes = 500\n",
    "explore_decay = 1\n",
    "explore_val = 0.01\n",
    "exit_level = 300\n",
    "exit_window = 100\n",
    "\n",
    "# initialize memory\n",
    "episode_update = 1\n",
    "memory_length = 2\n",
    "\n",
    "# load into instance of learner\n",
    "demo = learner.QLearner(gymname,dirname,savename,num_episodes=num_episodes,explore_decay=explore_decay,explore_val=explore_val,memory_length=memory_length,episode_update=episode_update,exit_level=exit_level,exit_window=exit_window)\n",
    "\n",
    "# initialize Q function\n",
    "layer_sizes = [50,50]\n",
    "alpha = 10**(-2)\n",
    "activation = 'relu'\n",
    "demo.initialize_Q(layer_sizes=layer_sizes,alpha=alpha,activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 of 500 complete,  explore val = 0.01, episode reward = 9.0, ave reward = 0.0\n",
      "episode 2 of 500 complete,  explore val = 0.01, episode reward = 12.0, ave reward = 0.1\n",
      "episode 3 of 500 complete,  explore val = 0.01, episode reward = 10.0, ave reward = 0.2\n",
      "episode 4 of 500 complete,  explore val = 0.01, episode reward = 9.0, ave reward = 0.3\n",
      "episode 5 of 500 complete,  explore val = 0.01, episode reward = 359.0, ave reward = 0.4\n",
      "episode 6 of 500 complete,  explore val = 0.01, episode reward = 84.0, ave reward = 4.0\n",
      "episode 7 of 500 complete,  explore val = 0.01, episode reward = 226.0, ave reward = 4.8\n",
      "episode 8 of 500 complete,  explore val = 0.01, episode reward = 83.0, ave reward = 7.1\n",
      "episode 9 of 500 complete,  explore val = 0.01, episode reward = 69.0, ave reward = 7.9\n",
      "episode 10 of 500 complete,  explore val = 0.01, episode reward = 34.0, ave reward = 8.6\n",
      "episode 11 of 500 complete,  explore val = 0.01, episode reward = 40.0, ave reward = 9.0\n",
      "episode 12 of 500 complete,  explore val = 0.01, episode reward = 47.0, ave reward = 9.4\n",
      "episode 13 of 500 complete,  explore val = 0.01, episode reward = 9.0, ave reward = 9.8\n",
      "episode 14 of 500 complete,  explore val = 0.01, episode reward = 51.0, ave reward = 9.9\n",
      "episode 15 of 500 complete,  explore val = 0.01, episode reward = 9.0, ave reward = 10.4\n",
      "episode 16 of 500 complete,  explore val = 0.01, episode reward = 9.0, ave reward = 10.5\n",
      "episode 17 of 500 complete,  explore val = 0.01, episode reward = 46.0, ave reward = 10.6\n",
      "episode 18 of 500 complete,  explore val = 0.01, episode reward = 38.0, ave reward = 11.1\n",
      "episode 19 of 500 complete,  explore val = 0.01, episode reward = 27.0, ave reward = 11.4\n",
      "episode 20 of 500 complete,  explore val = 0.01, episode reward = 65.0, ave reward = 11.7\n",
      "episode 21 of 500 complete,  explore val = 0.01, episode reward = 44.0, ave reward = 12.4\n",
      "episode 22 of 500 complete,  explore val = 0.01, episode reward = 67.0, ave reward = 12.8\n",
      "episode 23 of 500 complete,  explore val = 0.01, episode reward = 383.0, ave reward = 13.5\n",
      "episode 24 of 500 complete,  explore val = 0.01, episode reward = 74.0, ave reward = 17.3\n",
      "episode 25 of 500 complete,  explore val = 0.01, episode reward = 75.0, ave reward = 18.0\n",
      "episode 26 of 500 complete,  explore val = 0.01, episode reward = 49.0, ave reward = 18.8\n",
      "episode 27 of 500 complete,  explore val = 0.01, episode reward = 56.0, ave reward = 19.3\n",
      "episode 28 of 500 complete,  explore val = 0.01, episode reward = 40.0, ave reward = 19.8\n",
      "episode 29 of 500 complete,  explore val = 0.01, episode reward = 41.0, ave reward = 20.2\n",
      "episode 30 of 500 complete,  explore val = 0.01, episode reward = 56.0, ave reward = 20.6\n",
      "episode 31 of 500 complete,  explore val = 0.01, episode reward = 76.0, ave reward = 21.2\n",
      "episode 32 of 500 complete,  explore val = 0.01, episode reward = 13.0, ave reward = 22.0\n",
      "episode 33 of 500 complete,  explore val = 0.01, episode reward = 66.0, ave reward = 22.1\n",
      "episode 34 of 500 complete,  explore val = 0.01, episode reward = 216.0, ave reward = 22.8\n",
      "episode 35 of 500 complete,  explore val = 0.01, episode reward = 41.0, ave reward = 24.9\n",
      "episode 36 of 500 complete,  explore val = 0.01, episode reward = 107.0, ave reward = 25.3\n",
      "episode 37 of 500 complete,  explore val = 0.01, episode reward = 33.0, ave reward = 26.4\n",
      "episode 38 of 500 complete,  explore val = 0.01, episode reward = 36.0, ave reward = 26.7\n",
      "episode 39 of 500 complete,  explore val = 0.01, episode reward = 32.0, ave reward = 27.1\n",
      "episode 40 of 500 complete,  explore val = 0.01, episode reward = 8.0, ave reward = 27.4\n",
      "episode 41 of 500 complete,  explore val = 0.01, episode reward = 27.0, ave reward = 27.5\n",
      "episode 42 of 500 complete,  explore val = 0.01, episode reward = 64.0, ave reward = 27.8\n",
      "episode 43 of 500 complete,  explore val = 0.01, episode reward = 42.0, ave reward = 28.4\n",
      "episode 44 of 500 complete,  explore val = 0.01, episode reward = 43.0, ave reward = 28.8\n",
      "episode 45 of 500 complete,  explore val = 0.01, episode reward = 50.0, ave reward = 29.2\n",
      "episode 46 of 500 complete,  explore val = 0.01, episode reward = 73.0, ave reward = 29.8\n",
      "episode 47 of 500 complete,  explore val = 0.01, episode reward = 47.0, ave reward = 30.5\n",
      "episode 48 of 500 complete,  explore val = 0.01, episode reward = 87.0, ave reward = 31.0\n",
      "episode 49 of 500 complete,  explore val = 0.01, episode reward = 124.0, ave reward = 31.8\n",
      "episode 50 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 33.1\n",
      "episode 51 of 500 complete,  explore val = 0.01, episode reward = 68.0, ave reward = 38.1\n",
      "episode 52 of 500 complete,  explore val = 0.01, episode reward = 57.0, ave reward = 38.7\n",
      "episode 53 of 500 complete,  explore val = 0.01, episode reward = 223.0, ave reward = 39.3\n",
      "episode 54 of 500 complete,  explore val = 0.01, episode reward = 137.0, ave reward = 41.5\n",
      "episode 55 of 500 complete,  explore val = 0.01, episode reward = 283.0, ave reward = 42.9\n",
      "episode 56 of 500 complete,  explore val = 0.01, episode reward = 94.0, ave reward = 45.7\n",
      "episode 57 of 500 complete,  explore val = 0.01, episode reward = 145.0, ave reward = 46.7\n",
      "episode 58 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 48.1\n",
      "episode 59 of 500 complete,  explore val = 0.01, episode reward = 172.0, ave reward = 53.1\n",
      "episode 60 of 500 complete,  explore val = 0.01, episode reward = 118.0, ave reward = 54.8\n",
      "episode 61 of 500 complete,  explore val = 0.01, episode reward = 205.0, ave reward = 56.0\n",
      "episode 62 of 500 complete,  explore val = 0.01, episode reward = 127.0, ave reward = 58.1\n",
      "episode 63 of 500 complete,  explore val = 0.01, episode reward = 195.0, ave reward = 59.4\n",
      "episode 64 of 500 complete,  explore val = 0.01, episode reward = 177.0, ave reward = 61.3\n",
      "episode 65 of 500 complete,  explore val = 0.01, episode reward = 189.0, ave reward = 63.1\n",
      "episode 66 of 500 complete,  explore val = 0.01, episode reward = 122.0, ave reward = 65.0\n",
      "episode 67 of 500 complete,  explore val = 0.01, episode reward = 374.0, ave reward = 66.2\n",
      "episode 68 of 500 complete,  explore val = 0.01, episode reward = 183.0, ave reward = 69.9\n",
      "episode 69 of 500 complete,  explore val = 0.01, episode reward = 142.0, ave reward = 71.8\n",
      "episode 70 of 500 complete,  explore val = 0.01, episode reward = 175.0, ave reward = 73.2\n",
      "episode 71 of 500 complete,  explore val = 0.01, episode reward = 151.0, ave reward = 74.9\n",
      "episode 72 of 500 complete,  explore val = 0.01, episode reward = 149.0, ave reward = 76.4\n",
      "episode 73 of 500 complete,  explore val = 0.01, episode reward = 181.0, ave reward = 77.9\n",
      "episode 74 of 500 complete,  explore val = 0.01, episode reward = 139.0, ave reward = 79.7\n",
      "episode 75 of 500 complete,  explore val = 0.01, episode reward = 159.0, ave reward = 81.1\n",
      "episode 76 of 500 complete,  explore val = 0.01, episode reward = 129.0, ave reward = 82.7\n",
      "episode 77 of 500 complete,  explore val = 0.01, episode reward = 87.0, ave reward = 84.0\n",
      "episode 78 of 500 complete,  explore val = 0.01, episode reward = 133.0, ave reward = 84.9\n",
      "episode 79 of 500 complete,  explore val = 0.01, episode reward = 160.0, ave reward = 86.2\n",
      "episode 80 of 500 complete,  explore val = 0.01, episode reward = 145.0, ave reward = 87.8\n",
      "episode 81 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 89.2\n",
      "episode 82 of 500 complete,  explore val = 0.01, episode reward = 143.0, ave reward = 94.2\n",
      "episode 83 of 500 complete,  explore val = 0.01, episode reward = 185.0, ave reward = 95.7\n",
      "episode 84 of 500 complete,  explore val = 0.01, episode reward = 132.0, ave reward = 97.5\n",
      "episode 85 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 98.8\n",
      "episode 86 of 500 complete,  explore val = 0.01, episode reward = 90.0, ave reward = 103.8\n",
      "episode 87 of 500 complete,  explore val = 0.01, episode reward = 190.0, ave reward = 104.8\n",
      "episode 88 of 500 complete,  explore val = 0.01, episode reward = 88.0, ave reward = 106.6\n",
      "episode 89 of 500 complete,  explore val = 0.01, episode reward = 258.0, ave reward = 107.5\n",
      "episode 90 of 500 complete,  explore val = 0.01, episode reward = 106.0, ave reward = 110.1\n",
      "episode 91 of 500 complete,  explore val = 0.01, episode reward = 105.0, ave reward = 111.2\n",
      "episode 92 of 500 complete,  explore val = 0.01, episode reward = 39.0, ave reward = 112.2\n",
      "episode 93 of 500 complete,  explore val = 0.01, episode reward = 134.0, ave reward = 112.6\n",
      "episode 94 of 500 complete,  explore val = 0.01, episode reward = 111.0, ave reward = 114.0\n",
      "episode 95 of 500 complete,  explore val = 0.01, episode reward = 95.0, ave reward = 115.1\n",
      "episode 96 of 500 complete,  explore val = 0.01, episode reward = 90.0, ave reward = 116.0\n",
      "episode 97 of 500 complete,  explore val = 0.01, episode reward = 31.0, ave reward = 116.9\n",
      "episode 98 of 500 complete,  explore val = 0.01, episode reward = 37.0, ave reward = 117.2\n",
      "episode 99 of 500 complete,  explore val = 0.01, episode reward = 17.0, ave reward = 117.6\n",
      "episode 100 of 500 complete,  explore val = 0.01, episode reward = 174.0, ave reward = 117.8\n",
      "episode 101 of 500 complete,  explore val = 0.01, episode reward = 36.0, ave reward = 119.5\n",
      "episode 102 of 500 complete,  explore val = 0.01, episode reward = 63.0, ave reward = 119.8\n",
      "episode 103 of 500 complete,  explore val = 0.01, episode reward = 182.0, ave reward = 120.3\n",
      "episode 104 of 500 complete,  explore val = 0.01, episode reward = 157.0, ave reward = 122.0\n",
      "episode 105 of 500 complete,  explore val = 0.01, episode reward = 258.0, ave reward = 123.5\n",
      "episode 106 of 500 complete,  explore val = 0.01, episode reward = 218.0, ave reward = 122.5\n",
      "episode 107 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 123.8\n",
      "episode 108 of 500 complete,  explore val = 0.01, episode reward = 186.0, ave reward = 126.6\n",
      "episode 109 of 500 complete,  explore val = 0.01, episode reward = 381.0, ave reward = 127.6\n",
      "episode 110 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 130.7\n",
      "episode 111 of 500 complete,  explore val = 0.01, episode reward = 186.0, ave reward = 135.4\n",
      "episode 112 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 136.8\n",
      "episode 113 of 500 complete,  explore val = 0.01, episode reward = 219.0, ave reward = 141.4\n",
      "episode 114 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 143.4\n",
      "episode 115 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 147.9\n",
      "episode 116 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 152.8\n",
      "episode 117 of 500 complete,  explore val = 0.01, episode reward = 108.0, ave reward = 157.8\n",
      "episode 118 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 158.4\n",
      "episode 119 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 163.0\n",
      "episode 120 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 167.7\n",
      "episode 121 of 500 complete,  explore val = 0.01, episode reward = 27.0, ave reward = 172.1\n",
      "episode 122 of 500 complete,  explore val = 0.01, episode reward = 112.0, ave reward = 171.9\n",
      "episode 123 of 500 complete,  explore val = 0.01, episode reward = 10.0, ave reward = 172.4\n",
      "episode 124 of 500 complete,  explore val = 0.01, episode reward = 196.0, ave reward = 168.6\n",
      "episode 125 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 169.8\n",
      "episode 126 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 174.1\n",
      "episode 127 of 500 complete,  explore val = 0.01, episode reward = 125.0, ave reward = 178.6\n",
      "episode 128 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 179.3\n",
      "episode 129 of 500 complete,  explore val = 0.01, episode reward = 21.0, ave reward = 183.9\n",
      "episode 130 of 500 complete,  explore val = 0.01, episode reward = 285.0, ave reward = 183.7\n",
      "episode 131 of 500 complete,  explore val = 0.01, episode reward = 12.0, ave reward = 186.0\n",
      "episode 132 of 500 complete,  explore val = 0.01, episode reward = 197.0, ave reward = 185.4\n",
      "episode 133 of 500 complete,  explore val = 0.01, episode reward = 18.0, ave reward = 187.2\n",
      "episode 134 of 500 complete,  explore val = 0.01, episode reward = 20.0, ave reward = 186.7\n",
      "episode 135 of 500 complete,  explore val = 0.01, episode reward = 21.0, ave reward = 184.8\n",
      "episode 136 of 500 complete,  explore val = 0.01, episode reward = 25.0, ave reward = 184.6\n",
      "episode 137 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 183.7\n",
      "episode 138 of 500 complete,  explore val = 0.01, episode reward = 10.0, ave reward = 188.4\n",
      "episode 139 of 500 complete,  explore val = 0.01, episode reward = 67.0, ave reward = 188.1\n",
      "episode 140 of 500 complete,  explore val = 0.01, episode reward = 219.0, ave reward = 188.5\n",
      "episode 141 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 190.6\n",
      "episode 142 of 500 complete,  explore val = 0.01, episode reward = 84.0, ave reward = 195.3\n",
      "episode 143 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 195.5\n",
      "episode 144 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 200.1\n",
      "episode 145 of 500 complete,  explore val = 0.01, episode reward = 295.0, ave reward = 204.7\n",
      "episode 146 of 500 complete,  explore val = 0.01, episode reward = 500.0, ave reward = 207.1\n"
     ]
    }
   ],
   "source": [
    "demo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot total episode reward history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward_logname = 'reward_logs/' + savename + '.txt'\n",
    "plotter.plot_reward_history(reward_logname,window_length = exit_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
