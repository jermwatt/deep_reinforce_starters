{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learner for cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Q-Learning algo written in Keras for the cartpole\n",
    "\n",
    "- Batch version used here, Q function updated after each episode of simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os \n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "# import custom reinforcement library\n",
    "import reinforcement_library as reinlib\n",
    "\n",
    "# import cartpole + pytorch online q-learner\n",
    "learner = reinlib.deep_Q_Learning.qfitted_deepQlearning_keras\n",
    "plotter = reinlib.deep_Q_Learning.history_plotter\n",
    "\n",
    "# load in autoreload so any changes made to backend files mirrored in notebook\n",
    "# without the need to restart kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-Learner Algo can be loaded in from backend file by activating the command\n",
    "\n",
    "```learner.QLearner??```\n",
    "\n",
    "in a code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning setup interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# feed in current directory name and savename = experiment name\n",
    "dirname = os.getcwd()\n",
    "savename = 'qfitted_lunarlander_experiment_1'\n",
    "gymname = 'LunarLander-v2'\n",
    "\n",
    "# initialize Q Learn process\n",
    "num_episodes = 1000\n",
    "explore_decay = 1\n",
    "explore_val = 0.01\n",
    "exit_level = 200\n",
    "exit_window = 100\n",
    "\n",
    "# initialize memory\n",
    "episode_update = 1\n",
    "memory_length = 2\n",
    "\n",
    "# load into instance of learner\n",
    "demo = learner.QLearner(gymname,dirname,savename,num_episodes=num_episodes,explore_decay=explore_decay,explore_val=explore_val,memory_length=memory_length,episode_update=episode_update,exit_level=exit_level,exit_window=exit_window)\n",
    "\n",
    "# initialize Q function\n",
    "layer_sizes = [200,200]\n",
    "alpha = 10**(-2)\n",
    "activation = 'relu'\n",
    "demo.initialize_Q(layer_sizes=layer_sizes,alpha=alpha,activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 of 1000 complete,  explore val = 0.01, episode reward = -416.8, ave reward = 0.0\n",
      "episode 2 of 1000 complete,  explore val = 0.01, episode reward = -530.7, ave reward = -4.2\n",
      "episode 3 of 1000 complete,  explore val = 0.01, episode reward = -542.7, ave reward = -9.5\n",
      "episode 4 of 1000 complete,  explore val = 0.01, episode reward = -494.0, ave reward = -14.9\n",
      "episode 5 of 1000 complete,  explore val = 0.01, episode reward = -666.4, ave reward = -19.8\n",
      "episode 6 of 1000 complete,  explore val = 0.01, episode reward = -663.0, ave reward = -26.5\n",
      "episode 7 of 1000 complete,  explore val = 0.01, episode reward = -483.5, ave reward = -33.1\n",
      "episode 8 of 1000 complete,  explore val = 0.01, episode reward = -262.7, ave reward = -38.0\n",
      "episode 9 of 1000 complete,  explore val = 0.01, episode reward = -755.8, ave reward = -40.6\n",
      "episode 10 of 1000 complete,  explore val = 0.01, episode reward = -533.1, ave reward = -48.2\n",
      "episode 11 of 1000 complete,  explore val = 0.01, episode reward = -585.7, ave reward = -53.5\n",
      "episode 12 of 1000 complete,  explore val = 0.01, episode reward = -72.9, ave reward = -59.3\n",
      "episode 13 of 1000 complete,  explore val = 0.01, episode reward = 81.6, ave reward = -60.1\n",
      "episode 14 of 1000 complete,  explore val = 0.01, episode reward = -306.6, ave reward = -59.3\n",
      "episode 15 of 1000 complete,  explore val = 0.01, episode reward = -244.2, ave reward = -62.3\n",
      "episode 16 of 1000 complete,  explore val = 0.01, episode reward = -122.7, ave reward = -64.8\n",
      "episode 17 of 1000 complete,  explore val = 0.01, episode reward = -503.5, ave reward = -66.0\n",
      "episode 18 of 1000 complete,  explore val = 0.01, episode reward = -170.3, ave reward = -71.0\n",
      "episode 19 of 1000 complete,  explore val = 0.01, episode reward = 40.0, ave reward = -72.7\n",
      "episode 20 of 1000 complete,  explore val = 0.01, episode reward = -368.0, ave reward = -72.3\n",
      "episode 21 of 1000 complete,  explore val = 0.01, episode reward = -605.1, ave reward = -76.0\n",
      "episode 22 of 1000 complete,  explore val = 0.01, episode reward = -208.7, ave reward = -82.1\n",
      "episode 23 of 1000 complete,  explore val = 0.01, episode reward = -226.1, ave reward = -84.1\n",
      "episode 24 of 1000 complete,  explore val = 0.01, episode reward = -793.1, ave reward = -86.4\n",
      "episode 25 of 1000 complete,  explore val = 0.01, episode reward = -28.8, ave reward = -94.3\n",
      "episode 26 of 1000 complete,  explore val = 0.01, episode reward = -157.1, ave reward = -94.6\n",
      "episode 27 of 1000 complete,  explore val = 0.01, episode reward = -27.6, ave reward = -96.2\n",
      "episode 28 of 1000 complete,  explore val = 0.01, episode reward = 4.5, ave reward = -96.5\n",
      "episode 29 of 1000 complete,  explore val = 0.01, episode reward = -14.2, ave reward = -96.4\n",
      "episode 30 of 1000 complete,  explore val = 0.01, episode reward = -101.8, ave reward = -96.6\n",
      "episode 31 of 1000 complete,  explore val = 0.01, episode reward = -144.6, ave reward = -97.6\n",
      "episode 32 of 1000 complete,  explore val = 0.01, episode reward = -24.2, ave reward = -99.0\n",
      "episode 33 of 1000 complete,  explore val = 0.01, episode reward = 5.9, ave reward = -99.3\n",
      "episode 34 of 1000 complete,  explore val = 0.01, episode reward = -60.6, ave reward = -99.2\n",
      "episode 35 of 1000 complete,  explore val = 0.01, episode reward = -92.1, ave reward = -99.8\n",
      "episode 36 of 1000 complete,  explore val = 0.01, episode reward = -14.2, ave reward = -100.7\n",
      "episode 37 of 1000 complete,  explore val = 0.01, episode reward = -18.3, ave reward = -100.9\n",
      "episode 38 of 1000 complete,  explore val = 0.01, episode reward = -70.6, ave reward = -101.1\n",
      "episode 39 of 1000 complete,  explore val = 0.01, episode reward = -87.4, ave reward = -101.8\n",
      "episode 40 of 1000 complete,  explore val = 0.01, episode reward = -21.6, ave reward = -102.7\n",
      "episode 41 of 1000 complete,  explore val = 0.01, episode reward = 17.1, ave reward = -102.9\n",
      "episode 42 of 1000 complete,  explore val = 0.01, episode reward = -82.0, ave reward = -102.7\n",
      "episode 43 of 1000 complete,  explore val = 0.01, episode reward = -329.1, ave reward = -103.5\n",
      "episode 44 of 1000 complete,  explore val = 0.01, episode reward = -49.4, ave reward = -106.8\n",
      "episode 45 of 1000 complete,  explore val = 0.01, episode reward = -812.5, ave reward = -107.3\n",
      "episode 46 of 1000 complete,  explore val = 0.01, episode reward = 37.0, ave reward = -115.4\n",
      "episode 47 of 1000 complete,  explore val = 0.01, episode reward = -297.4, ave reward = -115.1\n",
      "episode 48 of 1000 complete,  explore val = 0.01, episode reward = -343.2, ave reward = -118.0\n",
      "episode 49 of 1000 complete,  explore val = 0.01, episode reward = -747.4, ave reward = -121.5\n",
      "episode 50 of 1000 complete,  explore val = 0.01, episode reward = -106.0, ave reward = -128.9\n",
      "episode 51 of 1000 complete,  explore val = 0.01, episode reward = -309.4, ave reward = -130.0\n",
      "episode 52 of 1000 complete,  explore val = 0.01, episode reward = -401.9, ave reward = -133.1\n",
      "episode 53 of 1000 complete,  explore val = 0.01, episode reward = -622.5, ave reward = -137.1\n",
      "episode 54 of 1000 complete,  explore val = 0.01, episode reward = 32.3, ave reward = -143.3\n",
      "episode 55 of 1000 complete,  explore val = 0.01, episode reward = -254.7, ave reward = -143.0\n",
      "episode 56 of 1000 complete,  explore val = 0.01, episode reward = -24.4, ave reward = -145.6\n",
      "episode 57 of 1000 complete,  explore val = 0.01, episode reward = -685.9, ave reward = -145.8\n",
      "episode 58 of 1000 complete,  explore val = 0.01, episode reward = -450.0, ave reward = -152.7\n",
      "episode 59 of 1000 complete,  explore val = 0.01, episode reward = -42.3, ave reward = -157.2\n",
      "episode 60 of 1000 complete,  explore val = 0.01, episode reward = -505.0, ave reward = -157.6\n",
      "episode 61 of 1000 complete,  explore val = 0.01, episode reward = -208.9, ave reward = -162.6\n",
      "episode 62 of 1000 complete,  explore val = 0.01, episode reward = -225.6, ave reward = -164.7\n",
      "episode 63 of 1000 complete,  explore val = 0.01, episode reward = -655.0, ave reward = -167.0\n",
      "episode 64 of 1000 complete,  explore val = 0.01, episode reward = -777.7, ave reward = -173.5\n",
      "episode 65 of 1000 complete,  explore val = 0.01, episode reward = -368.3, ave reward = -181.3\n",
      "episode 66 of 1000 complete,  explore val = 0.01, episode reward = -403.4, ave reward = -185.0\n",
      "episode 67 of 1000 complete,  explore val = 0.01, episode reward = -440.9, ave reward = -189.0\n",
      "episode 68 of 1000 complete,  explore val = 0.01, episode reward = -81.9, ave reward = -193.4\n",
      "episode 69 of 1000 complete,  explore val = 0.01, episode reward = -749.2, ave reward = -194.3\n",
      "episode 70 of 1000 complete,  explore val = 0.01, episode reward = -209.3, ave reward = -201.7\n",
      "episode 71 of 1000 complete,  explore val = 0.01, episode reward = -134.0, ave reward = -203.8\n",
      "episode 72 of 1000 complete,  explore val = 0.01, episode reward = -508.5, ave reward = -205.2\n",
      "episode 73 of 1000 complete,  explore val = 0.01, episode reward = -30.3, ave reward = -210.3\n",
      "episode 74 of 1000 complete,  explore val = 0.01, episode reward = -361.0, ave reward = -210.6\n",
      "episode 75 of 1000 complete,  explore val = 0.01, episode reward = -2.6, ave reward = -214.2\n",
      "episode 76 of 1000 complete,  explore val = 0.01, episode reward = -187.1, ave reward = -214.2\n",
      "episode 77 of 1000 complete,  explore val = 0.01, episode reward = -159.2, ave reward = -216.1\n",
      "episode 78 of 1000 complete,  explore val = 0.01, episode reward = -70.5, ave reward = -217.7\n",
      "episode 79 of 1000 complete,  explore val = 0.01, episode reward = -4.9, ave reward = -218.4\n",
      "episode 80 of 1000 complete,  explore val = 0.01, episode reward = -20.2, ave reward = -218.4\n",
      "episode 81 of 1000 complete,  explore val = 0.01, episode reward = -89.0, ave reward = -218.6\n"
     ]
    }
   ],
   "source": [
    "demo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot total episode reward history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward_logname = 'reward_logs/' + savename + '.txt'\n",
    "plotter.plot_reward_history(reward_logname,window_length = exit_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
