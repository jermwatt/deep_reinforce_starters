{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-Learning algo written in Python + Autograd for the cartpole\n",
    "\n",
    "- Online version used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os \n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "# import custom reinforcement library\n",
    "import reinforcement_library as reinlib\n",
    "\n",
    "# import cartpole + pytorch online q-learner\n",
    "learner = reinlib.deep_Q_Learning.online_cartpole_python_autograd\n",
    "plotter = reinlib.deep_Q_Learning.history_plotter\n",
    "\n",
    "# load in autoreload so any changes made to backend files mirrored in notebook\n",
    "# without the need to restart kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-Learner Algo can be loaded in from backend file by activating the command\n",
    "\n",
    "```learner.QLearner??```\n",
    "\n",
    "in a code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning setup interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# feed in current directory name and savename = experiment name\n",
    "dirname = os.getcwd()\n",
    "savename = 'online_cartpole_experiment_1'\n",
    "\n",
    "# initialize Q Learn process\n",
    "num_episodes = 500\n",
    "explore_decay = 1\n",
    "explore_val = 0.1\n",
    "\n",
    "# initialize memory\n",
    "replay_length = 100\n",
    "memory_length = 1000\n",
    "\n",
    "# load into instance of learner\n",
    "demo = learner.QLearner(dirname,savename,num_episodes=num_episodes,explore_decay=explore_decay,explore_val=explore_val,memory_length=memory_length,replay_length=replay_length)\n",
    "\n",
    "# initialize Q function\n",
    "layer_sizes = [25,25]\n",
    "alpha = 10**(-2)\n",
    "activation = 'relu'\n",
    "demo.initialize_Q(layer_sizes=layer_sizes,alpha=alpha,activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training episode 1 of 500 complete,  explore val = 0.995, episode reward = 12.0\n",
      "training episode 2 of 500 complete,  explore val = 0.99, episode reward = 14.0\n",
      "training episode 3 of 500 complete,  explore val = 0.985, episode reward = 13.0\n",
      "training episode 4 of 500 complete,  explore val = 0.98, episode reward = 33.0\n",
      "training episode 5 of 500 complete,  explore val = 0.975, episode reward = 16.0\n",
      "training episode 6 of 500 complete,  explore val = 0.97, episode reward = 15.0\n",
      "training episode 7 of 500 complete,  explore val = 0.966, episode reward = 15.0\n",
      "training episode 8 of 500 complete,  explore val = 0.961, episode reward = 38.0\n",
      "training episode 9 of 500 complete,  explore val = 0.956, episode reward = 33.0\n",
      "training episode 10 of 500 complete,  explore val = 0.951, episode reward = 23.0\n",
      "training episode 11 of 500 complete,  explore val = 0.946, episode reward = 44.0\n",
      "training episode 12 of 500 complete,  explore val = 0.942, episode reward = 16.0\n",
      "training episode 13 of 500 complete,  explore val = 0.937, episode reward = 14.0\n",
      "training episode 14 of 500 complete,  explore val = 0.932, episode reward = 44.0\n",
      "training episode 15 of 500 complete,  explore val = 0.928, episode reward = 32.0\n",
      "training episode 16 of 500 complete,  explore val = 0.923, episode reward = 10.0\n",
      "training episode 17 of 500 complete,  explore val = 0.918, episode reward = 18.0\n",
      "training episode 18 of 500 complete,  explore val = 0.914, episode reward = 20.0\n",
      "training episode 19 of 500 complete,  explore val = 0.909, episode reward = 28.0\n",
      "training episode 20 of 500 complete,  explore val = 0.905, episode reward = 29.0\n",
      "training episode 21 of 500 complete,  explore val = 0.9, episode reward = 15.0\n",
      "training episode 22 of 500 complete,  explore val = 0.896, episode reward = 42.0\n",
      "training episode 23 of 500 complete,  explore val = 0.891, episode reward = 17.0\n",
      "training episode 24 of 500 complete,  explore val = 0.887, episode reward = 13.0\n",
      "training episode 25 of 500 complete,  explore val = 0.882, episode reward = 30.0\n",
      "training episode 26 of 500 complete,  explore val = 0.878, episode reward = 10.0\n",
      "training episode 27 of 500 complete,  explore val = 0.873, episode reward = 23.0\n",
      "training episode 28 of 500 complete,  explore val = 0.869, episode reward = 14.0\n",
      "training episode 29 of 500 complete,  explore val = 0.865, episode reward = 9.0\n",
      "training episode 30 of 500 complete,  explore val = 0.86, episode reward = 11.0\n",
      "training episode 31 of 500 complete,  explore val = 0.856, episode reward = 21.0\n",
      "training episode 32 of 500 complete,  explore val = 0.852, episode reward = 16.0\n",
      "training episode 33 of 500 complete,  explore val = 0.848, episode reward = 34.0\n",
      "training episode 34 of 500 complete,  explore val = 0.843, episode reward = 34.0\n",
      "training episode 35 of 500 complete,  explore val = 0.839, episode reward = 13.0\n",
      "training episode 36 of 500 complete,  explore val = 0.835, episode reward = 34.0\n",
      "training episode 37 of 500 complete,  explore val = 0.831, episode reward = 17.0\n",
      "training episode 38 of 500 complete,  explore val = 0.827, episode reward = 46.0\n",
      "training episode 39 of 500 complete,  explore val = 0.822, episode reward = 20.0\n",
      "training episode 40 of 500 complete,  explore val = 0.818, episode reward = 11.0\n",
      "training episode 41 of 500 complete,  explore val = 0.814, episode reward = 19.0\n",
      "training episode 42 of 500 complete,  explore val = 0.81, episode reward = 37.0\n",
      "training episode 43 of 500 complete,  explore val = 0.806, episode reward = 26.0\n",
      "training episode 44 of 500 complete,  explore val = 0.802, episode reward = 68.0\n",
      "training episode 45 of 500 complete,  explore val = 0.798, episode reward = 21.0\n",
      "training episode 46 of 500 complete,  explore val = 0.794, episode reward = 21.0\n",
      "training episode 47 of 500 complete,  explore val = 0.79, episode reward = 55.0\n",
      "training episode 48 of 500 complete,  explore val = 0.786, episode reward = 25.0\n",
      "training episode 49 of 500 complete,  explore val = 0.782, episode reward = 60.0\n",
      "training episode 50 of 500 complete,  explore val = 0.778, episode reward = 21.0\n",
      "training episode 51 of 500 complete,  explore val = 0.774, episode reward = 33.0\n",
      "training episode 52 of 500 complete,  explore val = 0.771, episode reward = 21.0\n",
      "training episode 53 of 500 complete,  explore val = 0.767, episode reward = 24.0\n",
      "training episode 54 of 500 complete,  explore val = 0.763, episode reward = 14.0\n",
      "training episode 55 of 500 complete,  explore val = 0.759, episode reward = 24.0\n",
      "training episode 56 of 500 complete,  explore val = 0.755, episode reward = 108.0\n",
      "training episode 57 of 500 complete,  explore val = 0.751, episode reward = 25.0\n",
      "training episode 58 of 500 complete,  explore val = 0.748, episode reward = 23.0\n",
      "training episode 59 of 500 complete,  explore val = 0.744, episode reward = 72.0\n",
      "training episode 60 of 500 complete,  explore val = 0.74, episode reward = 16.0\n",
      "training episode 61 of 500 complete,  explore val = 0.737, episode reward = 44.0\n",
      "training episode 62 of 500 complete,  explore val = 0.733, episode reward = 20.0\n",
      "training episode 63 of 500 complete,  explore val = 0.729, episode reward = 27.0\n",
      "training episode 64 of 500 complete,  explore val = 0.726, episode reward = 11.0\n",
      "training episode 65 of 500 complete,  explore val = 0.722, episode reward = 20.0\n",
      "training episode 66 of 500 complete,  explore val = 0.718, episode reward = 22.0\n",
      "training episode 67 of 500 complete,  explore val = 0.715, episode reward = 31.0\n",
      "training episode 68 of 500 complete,  explore val = 0.711, episode reward = 12.0\n",
      "training episode 69 of 500 complete,  explore val = 0.708, episode reward = 119.0\n",
      "training episode 70 of 500 complete,  explore val = 0.704, episode reward = 13.0\n",
      "training episode 71 of 500 complete,  explore val = 0.701, episode reward = 49.0\n",
      "training episode 72 of 500 complete,  explore val = 0.697, episode reward = 39.0\n",
      "training episode 73 of 500 complete,  explore val = 0.694, episode reward = 44.0\n",
      "training episode 74 of 500 complete,  explore val = 0.69, episode reward = 29.0\n",
      "training episode 75 of 500 complete,  explore val = 0.687, episode reward = 41.0\n",
      "training episode 76 of 500 complete,  explore val = 0.683, episode reward = 79.0\n",
      "training episode 77 of 500 complete,  explore val = 0.68, episode reward = 41.0\n",
      "training episode 78 of 500 complete,  explore val = 0.676, episode reward = 72.0\n",
      "training episode 79 of 500 complete,  explore val = 0.673, episode reward = 17.0\n",
      "training episode 80 of 500 complete,  explore val = 0.67, episode reward = 14.0\n",
      "training episode 81 of 500 complete,  explore val = 0.666, episode reward = 10.0\n",
      "training episode 82 of 500 complete,  explore val = 0.663, episode reward = 19.0\n",
      "training episode 83 of 500 complete,  explore val = 0.66, episode reward = 11.0\n",
      "training episode 84 of 500 complete,  explore val = 0.656, episode reward = 44.0\n",
      "training episode 85 of 500 complete,  explore val = 0.653, episode reward = 18.0\n",
      "training episode 86 of 500 complete,  explore val = 0.65, episode reward = 118.0\n",
      "training episode 87 of 500 complete,  explore val = 0.647, episode reward = 116.0\n"
     ]
    }
   ],
   "source": [
    "demo.train(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot total episode reward history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward_logname = 'reward_logs/' + savename + '.txt'\n",
    "plotter.plot_reward_history(reward_logname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
