{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learner for lunar lander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-Learning algo written in PyTorch for the cartpole\n",
    "\n",
    "- Batch version used here, Q function updated after each episode of simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os \n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "# import custom reinforcement library\n",
    "import reinforcement_library as reinlib\n",
    "\n",
    "# import cartpole + pytorch online q-learner\n",
    "learner = reinlib.deep_Q_Learning.qfitted_deepQlearning_pytorch\n",
    "plotter = reinlib.deep_Q_Learning.history_plotter\n",
    "\n",
    "# load in autoreload so any changes made to backend files mirrored in notebook\n",
    "# without the need to restart kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-Learner Algo can be loaded in from backend file by activating the command\n",
    "\n",
    "```learner.QLearner??```\n",
    "\n",
    "in a code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning setup interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# feed in current directory name and savename = experiment name\n",
    "dirname = os.getcwd()\n",
    "savename = 'qfitted_cartpole_experiment_1'\n",
    "gymname = 'LunarLander-v2'\n",
    "\n",
    "# initialize Q Learn process\n",
    "num_episodes = 1000\n",
    "explore_decay = 1\n",
    "explore_val = 0.01\n",
    "exit_level = 200\n",
    "exit_window = 10\n",
    "\n",
    "# initialize memory\n",
    "episode_update = 1\n",
    "memory_length = 1\n",
    "\n",
    "# load into instance of learner\n",
    "demo = learner.QLearner(gymname,dirname,savename,num_episodes=num_episodes,explore_decay=explore_decay,explore_val=explore_val,memory_length=memory_length,episode_update=episode_update,exit_level=exit_level,exit_window=exit_window)\n",
    "\n",
    "# initialize Q function\n",
    "layer_sizes = [300,300]\n",
    "alpha = 10**(-2)\n",
    "activation = 'relu'\n",
    "demo.initialize_Q(layer_sizes=layer_sizes,alpha=alpha,activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 of 1000 complete,  explore val = 0.01, episode reward = -539.5, ave reward = -53.9\n",
      "episode 2 of 1000 complete,  explore val = 0.01, episode reward = -1026.1, ave reward = -156.6\n",
      "episode 3 of 1000 complete,  explore val = 0.01, episode reward = -187.9, ave reward = -175.3\n",
      "episode 4 of 1000 complete,  explore val = 0.01, episode reward = -576.4, ave reward = -233.0\n",
      "episode 5 of 1000 complete,  explore val = 0.01, episode reward = -170.9, ave reward = -250.1\n",
      "episode 6 of 1000 complete,  explore val = 0.01, episode reward = -797.0, ave reward = -329.8\n",
      "episode 7 of 1000 complete,  explore val = 0.01, episode reward = -961.4, ave reward = -425.9\n",
      "episode 8 of 1000 complete,  explore val = 0.01, episode reward = -359.8, ave reward = -461.9\n",
      "episode 9 of 1000 complete,  explore val = 0.01, episode reward = -452.9, ave reward = -507.2\n",
      "episode 10 of 1000 complete,  explore val = 0.01, episode reward = -178.6, ave reward = -525.0\n",
      "episode 11 of 1000 complete,  explore val = 0.01, episode reward = -226.6, ave reward = -493.8\n",
      "episode 12 of 1000 complete,  explore val = 0.01, episode reward = -210.7, ave reward = -412.2\n",
      "episode 13 of 1000 complete,  explore val = 0.01, episode reward = -160.0, ave reward = -409.4\n",
      "episode 14 of 1000 complete,  explore val = 0.01, episode reward = -170.8, ave reward = -368.9\n",
      "episode 15 of 1000 complete,  explore val = 0.01, episode reward = -144.3, ave reward = -366.2\n",
      "episode 16 of 1000 complete,  explore val = 0.01, episode reward = -168.7, ave reward = -303.4\n",
      "episode 17 of 1000 complete,  explore val = 0.01, episode reward = -143.9, ave reward = -221.6\n",
      "episode 18 of 1000 complete,  explore val = 0.01, episode reward = -165.3, ave reward = -202.2\n",
      "episode 19 of 1000 complete,  explore val = 0.01, episode reward = -359.4, ave reward = -192.8\n",
      "episode 20 of 1000 complete,  explore val = 0.01, episode reward = -56.1, ave reward = -180.6\n",
      "episode 21 of 1000 complete,  explore val = 0.01, episode reward = -154.4, ave reward = -173.4\n",
      "episode 22 of 1000 complete,  explore val = 0.01, episode reward = -158.8, ave reward = -168.2\n",
      "episode 23 of 1000 complete,  explore val = 0.01, episode reward = -61.4, ave reward = -158.3\n",
      "episode 24 of 1000 complete,  explore val = 0.01, episode reward = -478.2, ave reward = -189.0\n",
      "episode 25 of 1000 complete,  explore val = 0.01, episode reward = -225.0, ave reward = -197.1\n",
      "episode 26 of 1000 complete,  explore val = 0.01, episode reward = -49.1, ave reward = -185.2\n",
      "episode 27 of 1000 complete,  explore val = 0.01, episode reward = -257.2, ave reward = -196.5\n",
      "episode 28 of 1000 complete,  explore val = 0.01, episode reward = -212.3, ave reward = -201.2\n",
      "episode 29 of 1000 complete,  explore val = 0.01, episode reward = -159.0, ave reward = -181.1\n",
      "episode 30 of 1000 complete,  explore val = 0.01, episode reward = -360.2, ave reward = -211.6\n",
      "episode 31 of 1000 complete,  explore val = 0.01, episode reward = -153.4, ave reward = -211.5\n",
      "episode 32 of 1000 complete,  explore val = 0.01, episode reward = -226.0, ave reward = -218.2\n",
      "episode 33 of 1000 complete,  explore val = 0.01, episode reward = -170.6, ave reward = -229.1\n",
      "episode 34 of 1000 complete,  explore val = 0.01, episode reward = -203.6, ave reward = -201.6\n",
      "episode 35 of 1000 complete,  explore val = 0.01, episode reward = -171.4, ave reward = -196.3\n",
      "episode 36 of 1000 complete,  explore val = 0.01, episode reward = -170.3, ave reward = -208.4\n",
      "episode 37 of 1000 complete,  explore val = 0.01, episode reward = -155.2, ave reward = -198.2\n",
      "episode 38 of 1000 complete,  explore val = 0.01, episode reward = -341.3, ave reward = -211.1\n",
      "episode 39 of 1000 complete,  explore val = 0.01, episode reward = -300.2, ave reward = -225.2\n",
      "episode 40 of 1000 complete,  explore val = 0.01, episode reward = -655.4, ave reward = -254.7\n",
      "episode 41 of 1000 complete,  explore val = 0.01, episode reward = -171.5, ave reward = -256.5\n",
      "episode 42 of 1000 complete,  explore val = 0.01, episode reward = -285.9, ave reward = -262.5\n",
      "episode 43 of 1000 complete,  explore val = 0.01, episode reward = -275.1, ave reward = -273.0\n",
      "episode 44 of 1000 complete,  explore val = 0.01, episode reward = -348.6, ave reward = -287.5\n",
      "episode 45 of 1000 complete,  explore val = 0.01, episode reward = -429.0, ave reward = -313.2\n",
      "episode 46 of 1000 complete,  explore val = 0.01, episode reward = -894.1, ave reward = -385.6\n",
      "episode 47 of 1000 complete,  explore val = 0.01, episode reward = -149.4, ave reward = -385.0\n",
      "episode 48 of 1000 complete,  explore val = 0.01, episode reward = -160.5, ave reward = -367.0\n",
      "episode 49 of 1000 complete,  explore val = 0.01, episode reward = -214.0, ave reward = -358.3\n",
      "episode 50 of 1000 complete,  explore val = 0.01, episode reward = -198.0, ave reward = -312.6\n",
      "episode 51 of 1000 complete,  explore val = 0.01, episode reward = -174.0, ave reward = -312.8\n",
      "episode 52 of 1000 complete,  explore val = 0.01, episode reward = -197.9, ave reward = -304.0\n",
      "episode 53 of 1000 complete,  explore val = 0.01, episode reward = -175.8, ave reward = -294.1\n",
      "episode 54 of 1000 complete,  explore val = 0.01, episode reward = -157.0, ave reward = -275.0\n",
      "episode 55 of 1000 complete,  explore val = 0.01, episode reward = -206.9, ave reward = -252.7\n",
      "episode 56 of 1000 complete,  explore val = 0.01, episode reward = -176.9, ave reward = -181.0\n",
      "episode 57 of 1000 complete,  explore val = 0.01, episode reward = -170.0, ave reward = -183.1\n",
      "episode 58 of 1000 complete,  explore val = 0.01, episode reward = -170.5, ave reward = -184.1\n",
      "episode 59 of 1000 complete,  explore val = 0.01, episode reward = -180.9, ave reward = -180.8\n",
      "episode 60 of 1000 complete,  explore val = 0.01, episode reward = -138.8, ave reward = -174.9\n",
      "episode 61 of 1000 complete,  explore val = 0.01, episode reward = -217.5, ave reward = -179.2\n",
      "episode 62 of 1000 complete,  explore val = 0.01, episode reward = -166.6, ave reward = -176.1\n",
      "episode 63 of 1000 complete,  explore val = 0.01, episode reward = -183.9, ave reward = -176.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3199fc223f58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jovyan/work/temp_junk/reinforcement_library/deep_Q_Learning/qfitted_deepQlearning_pytorch.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# save latest weights from this episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;31m# if the total episode reward is greater than exit level for 10 consecutive episodes, break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/work/temp_junk/reinforcement_library/deep_Q_Learning/qfitted_deepQlearning_pytorch.py\u001b[0m in \u001b[0;36mupdate_log\u001b[0;34m(self, logname, update)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                 \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;31m##### functions for creating / updating Q #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/torch/storage.py\u001b[0m in \u001b[0;36m__reduce__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sizeof__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/torch/storage.py\u001b[0m in \u001b[0;36mtolist\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;34m\"\"\"Returns a list containing the elements of this storage\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/torch/storage.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;34m\"\"\"Returns a list containing the elements of this storage\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/torch/storage.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__copy__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "demo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot total episode reward history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reward_logname = 'reward_logs/' + savename + '.txt'\n",
    "plotter.plot_reward_history(reward_logname,window_length = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
