{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "'''\n",
    "PyTorch does not the same abstract API Keras does for optimization, in particular\n",
    "while it does contain an array of advanced first order methods like e.g., RMSprop\n",
    "one needs to construct one's own looping structure in order to employ it properly. \n",
    "Below the class My_Opt does just this.  \n",
    "'''\n",
    "class My_Opt:\n",
    "    def __init__(self,model,cost):\n",
    "        self.model = model\n",
    "        self.cost = cost\n",
    "        learning_rate = 10**(-2)\n",
    "        self.optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # record history\n",
    "        self.cost_history = []\n",
    "        self.weight_history = []\n",
    "        \n",
    "    def fit(self,x,y,beta,max_its,lr):\n",
    "        # update learning rate\n",
    "        self.optimizer.param_groups[0]['lr'] = lr\n",
    "        \n",
    "        for t in range(max_its):\n",
    "            # Forward pass: compute predicted y by passing x to the model.\n",
    "            y_pred = self.model(x.float())\n",
    "\n",
    "            # Compute and print loss value\n",
    "            # weight each point differently given beta\n",
    "            cost_val = 0\n",
    "            for i in range(len(beta)):\n",
    "                inp = y_pred[i]\n",
    "                inp = inp.view(1,len(inp)).double()\n",
    "                out = y[i]\n",
    "                out = out.view(1,).long()\n",
    "                bet = beta[i]     \n",
    "                cost_val += bet*self.cost(inp,out)\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            if len(self.cost_history) == 0 or t > 0:\n",
    "                # store weight and cost history\n",
    "                self.cost_history.append(cost_val.data.item())\n",
    "                self.weight_history.append(self.optimizer.param_groups[0]['params'])\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model\n",
    "            # parameters\n",
    "            cost_val.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its\n",
    "            # parameters\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # collect last input / weights\n",
    "        y_pred = self.model(x.float())\n",
    "        cost_val = 0\n",
    "        for i in range(len(beta)):\n",
    "            inp = y_pred[i]\n",
    "            inp = inp.view(1,len(inp)).double()\n",
    "            out = y[i]\n",
    "            out = out.view(1,).long()\n",
    "            bet = beta[i]     \n",
    "            cost_val += bet*self.cost(inp,out)\n",
    "                \n",
    "        self.cost_history.append(cost_val.data.item())\n",
    "        self.weight_history.append(self.optimizer.param_groups[0]['params'])\n",
    "\n",
    "class PG():\n",
    "    # load in simulator, initialize global variables\n",
    "    def __init__(self,simulator,savename,**kwargs):\n",
    "        # make simulator global\n",
    "        self.simulator = simulator\n",
    "        \n",
    "        # PG learn params\n",
    "        self.explore_val = 1\n",
    "        self.explore_decay = 0.99\n",
    "        self.num_episodes = 500        \n",
    "        self.gamma = 1\n",
    "        \n",
    "        if \"gamma\" in kwargs:   \n",
    "            self.gamma = args['gamma']\n",
    "        if 'explore_val' in kwargs:\n",
    "            self.explore_val = kwargs['explore_val']\n",
    "        if 'explore_decay' in kwargs:\n",
    "            self.explore_decay = kwargs['explore_decay']\n",
    "        if 'num_episodes' in kwargs:\n",
    "            self.num_episodes = kwargs['num_episodes']\n",
    "            \n",
    "        # other training variables\n",
    "        self.num_actions = self.simulator.action_space.n\n",
    "        state = self.simulator.reset()    \n",
    "        self.state_dim = np.size(state)\n",
    "        self.training_reward = []\n",
    "        \n",
    "        # setup memory params\n",
    "        self.memory_length = 10     # length of memory replay (in episodes)\n",
    "        self.episode_update = 1     # when to update (in episodes)\n",
    "        self.memory = []\n",
    "        if 'memory_length' in kwargs:\n",
    "            self.memory_length = kwargs['memory_length']\n",
    "        if 'episode_update' in kwargs:\n",
    "            self.episode_update = kwargs['episode_update']\n",
    "            \n",
    "        ### initialize logs ###\n",
    "        # create text file for training log\n",
    "        self.logname = 'training_logs/' + savename + '.txt'\n",
    "        self.reward_logname = 'reward_logs/' + savename + '.txt'\n",
    "        self.weight_name = 'saved_model_weights/' + savename + '.pkl' \n",
    "        self.model_name = 'models/' + savename + '.json'\n",
    "\n",
    "        self.init_log(self.logname)\n",
    "        self.init_log(self.reward_logname)\n",
    "        self.init_log(self.weight_name)\n",
    "        self.init_log(self.model_name)\n",
    "     \n",
    "    ##### logging functions #####\n",
    "    def init_log(self,logname):\n",
    "        # delete log if old version exists\n",
    "        if os.path.exists(logname): \n",
    "            os.remove(logname)\n",
    "            \n",
    "    def update_log(self,logname,update):\n",
    "        if type(update) == str:\n",
    "            logfile = open(logname, \"a\")\n",
    "            logfile.write(update)\n",
    "            logfile.close() \n",
    "#         else:\n",
    "#             weights = []\n",
    "#             if os.path.exists(logname):\n",
    "#                 with open(logname,'rb') as rfp: \n",
    "#                     weights = pickle.load(rfp)\n",
    "#             weights.append(update)\n",
    "\n",
    "#             with open(logname,'wb') as wfp:\n",
    "#                 pickle.dump(weights, wfp)\n",
    "    \n",
    "    ##### functions for creating / updating Q #####\n",
    "    def initialize_Q(self,**kwargs):\n",
    "        # default parameters for network\n",
    "        layer_sizes = [10,10]      # two hidden layers, 10 units each, by default\n",
    "        activation = 'relu'\n",
    "        if 'layer_sizes' in kwargs:\n",
    "            layer_sizes = kwargs['layer_sizes']\n",
    "        if 'activation' in kwargs:\n",
    "            activation = kwargs['activation']\n",
    "\n",
    "        # default parameters for optimizer - reset by hand\n",
    "        self.lr = 10**(-2)\n",
    "        if 'alpha' in kwargs:\n",
    "            self.lr = kwargs['alpha']\n",
    "\n",
    "        # input / output sizes of network\n",
    "        input_dim = self.state_dim\n",
    "        output_dim = self.num_actions\n",
    "\n",
    "        # Use the nn package to define our model and loss function\n",
    "        self.model = torch.nn.Sequential()\n",
    "\n",
    "        # add input layer\n",
    "        self.model.add_module('linear ' + str(0),torch.nn.Linear(input_dim,layer_sizes[0]))\n",
    "        if activation == 'relu':\n",
    "            self.model.add_module('activation ' + str(0),torch.nn.ReLU())\n",
    "        if activation == 'tanh':\n",
    "            self.model.add_module('activation ' + str(0),torch.nn.Tanh())\n",
    "\n",
    "        # add hidden layers\n",
    "        for i in range(1,len(layer_sizes)):\n",
    "            U = layer_sizes[i-1]\n",
    "            V = layer_sizes[i]\n",
    "            self.model.add_module('linear ' + str(i),torch.nn.Linear(U,V))\n",
    "            if activation == 'relu':\n",
    "                self.model.add_module('activation ' + str(i),torch.nn.ReLU())\n",
    "            if activation == 'tanh':\n",
    "                self.model.add_module('activation ' + str(i),torch.nn.Tanh())\n",
    "\n",
    "        # add output layer\n",
    "        self.model.add_module('linear ' + str(len(layer_sizes)),torch.nn.Linear(layer_sizes[-1], output_dim))\n",
    "                \n",
    "        # define cost function\n",
    "        self.cost = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "        \n",
    "        # setup optimizer - using hand-made optimizer class + RMSprop\n",
    "        self.opt = My_Opt(self.model,self.cost)\n",
    "\n",
    "        # initialize Q\n",
    "        self.Q = self.opt.model.forward\n",
    "        \n",
    "    # compute long term rewards for weighted gradient descent steps\n",
    "    def create_weights(self,episode_data):\n",
    "        # create long term reward simulator\n",
    "        rewards = []\n",
    "        for j in range(len(episode_data)):\n",
    "            sample = episode_data[j]\n",
    "            reward = sample[3]\n",
    "            rewards.append(reward)\n",
    "\n",
    "        # build long term rewards for each step\n",
    "        beta = np.zeros_like(np.array(rewards))\n",
    "        longterm_reward = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            longterm_reward = rewards[t] + self.gamma*longterm_reward\n",
    "            beta[t] = longterm_reward\n",
    "        return beta\n",
    "    \n",
    "    # update Q function\n",
    "    def update_control(self):        \n",
    "        # generate q_values based on most recent Q\n",
    "        a_vals = []\n",
    "        states = []\n",
    "        for i in range(len(self.memory)):    \n",
    "            # get episode_data\n",
    "            episode_data = self.memory[i]\n",
    "            \n",
    "            # create long-term rewards (weights for gradient descent step)\n",
    "            beta = self.create_weights(episode_data)\n",
    "            \n",
    "            # create input/output data for gradient descent step\n",
    "            s_in = []\n",
    "            a_vals = []\n",
    "            for j in range(len(episode_data)):\n",
    "                # get next sample of episode\n",
    "                sample = episode_data[j]\n",
    "                state = sample[0]\n",
    "                action = sample[2]\n",
    "                \n",
    "                # strip sample for parts\n",
    "                s_in.append(state.T)\n",
    "                a_vals.append(action)\n",
    "\n",
    "            # convert lists to torch tensors classifier\n",
    "            s_in = np.array(s_in).T\n",
    "            s_in = s_in[0,:,:]\n",
    "            s_in = torch.from_numpy(s_in)\n",
    "            s_in = Variable(s_in,requires_grad = False)\n",
    "            \n",
    "            a_vals = np.array(a_vals)\n",
    "            a_vals = torch.from_numpy(a_vals)\n",
    "            a_vals = Variable(a_vals,requires_grad = False)\n",
    "        \n",
    "            beta = np.array(beta).T\n",
    "            beta = torch.from_numpy(beta)\n",
    "            beta = Variable(beta,requires_grad = False)\n",
    "            \n",
    "            # take descent step\n",
    "            self.opt.fit(s_in.t(),a_vals,beta,max_its = 1,lr=self.lr)\n",
    "\n",
    "        # update Q based on regressor updates\n",
    "        self.Q = self.opt.model.forward\n",
    "        \n",
    "    ##### functions for adjusting replay memory #####\n",
    "    # update memory - add sample to list, remove oldest samples \n",
    "    def update_memory(self,episode_data):\n",
    "        # add most recent trial data to memory\n",
    "        self.memory.append(episode_data)\n",
    "\n",
    "        # clip memory if it gets too long    \n",
    "        num_episodes = len(self.memory)\n",
    "        if num_episodes >= self.memory_length:    \n",
    "            num_delete = num_episodes - self.memory_length\n",
    "            self.memory[:num_delete] = []\n",
    "    \n",
    "    ##### PG Learning functionality #####    \n",
    "    # state normalizer\n",
    "    def state_normalizer(self,states):\n",
    "        states = np.array(states)[np.newaxis,:]\n",
    "        return states\n",
    "    \n",
    "    # choose next action\n",
    "    def choose_action(self,state):\n",
    "        # pick action at random\n",
    "        p = np.random.rand(1)   \n",
    "        action = np.random.randint(self.num_actions)\n",
    "            \n",
    "        # pick action based on exploiting - after memory full\n",
    "        state = Variable(torch.from_numpy(state),requires_grad=False)        \n",
    "        qs = self.Q(state.float()).detach().numpy()\n",
    "        if p > self.explore_val:\n",
    "            action = np.argmax(qs)\n",
    "        return action\n",
    "    \n",
    "    # main training function\n",
    "    def train(self,**kwargs):\n",
    "        ### start main PG loop ###\n",
    "        for n in range(self.num_episodes): \n",
    "            # pick this episode's starting position - randomly initialize from f_system\n",
    "            state = self.simulator.reset()    \n",
    "            state = self.state_normalizer(state)\n",
    "            total_episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            # get out exploit parameter for this episode\n",
    "            if self.explore_val > 0.01:\n",
    "                self.explore_val *= self.explore_decay\n",
    "                    \n",
    "            # run episode\n",
    "            step = 0\n",
    "            episode_data = []\n",
    "            while done == False and step < 500:    \n",
    "                # choose next action\n",
    "                action = self.choose_action(state) \n",
    "    \n",
    "                # transition to next state, get associated reward\n",
    "                next_state,reward,done,info = self.simulator.step(action)  \n",
    "                next_state = self.state_normalizer(next_state)\n",
    "\n",
    "                # store data for transition after episode ends\n",
    "                episode_data.append([state,next_state,action,reward,done])\n",
    "\n",
    "                # update total reward from this episode\n",
    "                total_episode_reward+=reward\n",
    "                state = copy.deepcopy(next_state)\n",
    "                step += 1\n",
    "                  \n",
    "            # update memory with this episode's data\n",
    "            self.update_memory(episode_data)\n",
    "            \n",
    "            # update Q function\n",
    "            if np.mod(n,self.episode_update) == 0:\n",
    "                self.update_control()  \n",
    "                  \n",
    "            ### print out updates ###\n",
    "            update = 'training episode ' + str(n+1) +  ' of ' + str(self.num_episodes) + ' complete, ' +  ' explore val = ' + str(np.round(self.explore_val,3)) + ', episode reward = ' + str(np.round(total_episode_reward,2)) \n",
    "\n",
    "            self.update_log(self.logname,update + '\\n')\n",
    "            print (update)\n",
    "\n",
    "            update = str(total_episode_reward) + '\\n'\n",
    "            self.update_log(self.reward_logname,update)\n",
    "\n",
    "            ### store this episode's computation time and training reward history\n",
    "            self.training_reward.append(total_episode_reward)\n",
    "\n",
    "            # save latest weights from this episode \n",
    "            update = self.opt.weight_history[-1]\n",
    "            self.update_log(self.weight_name,update)\n",
    "                            \n",
    "        ### save weights ###\n",
    "        update = 'q-learning algorithm complete'\n",
    "        self.update_log(self.logname,update + '\\n')\n",
    "        print (update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import gym\n",
    "\n",
    "# savename\n",
    "savename = 'cartpole_experiment_1'\n",
    "\n",
    "# initialize simulator\n",
    "simulator = gym.make('CartPole-v1') \n",
    "\n",
    "# initialize PG Learn process\n",
    "num_episodes = 500\n",
    "explore_decay = 0.995\n",
    "explore_val = 1\n",
    "\n",
    "# initialize memory\n",
    "episode_update = 1\n",
    "memory_length = 1\n",
    "\n",
    "# load into instance of learner\n",
    "demo = PG(simulator,savename,num_episodes=num_episodes,explore_decay=explore_decay,explore_val=explore_val,memory_length=memory_length,episode_update=episode_update)\n",
    "\n",
    "# initialize Q function\n",
    "layer_sizes = [100]\n",
    "alpha = 10**(-2)\n",
    "activation = 'relu'\n",
    "demo.initialize_Q(layer_sizes=layer_sizes,alpha=alpha,activation=activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training episode 1 of 500 complete,  explore val = 0.995, episode reward = 51.0\n",
      "training episode 2 of 500 complete,  explore val = 0.99, episode reward = 25.0\n",
      "training episode 3 of 500 complete,  explore val = 0.985, episode reward = 68.0\n",
      "training episode 4 of 500 complete,  explore val = 0.98, episode reward = 34.0\n",
      "training episode 5 of 500 complete,  explore val = 0.975, episode reward = 19.0\n",
      "training episode 6 of 500 complete,  explore val = 0.97, episode reward = 18.0\n",
      "training episode 7 of 500 complete,  explore val = 0.966, episode reward = 16.0\n",
      "training episode 8 of 500 complete,  explore val = 0.961, episode reward = 24.0\n",
      "training episode 9 of 500 complete,  explore val = 0.956, episode reward = 42.0\n",
      "training episode 10 of 500 complete,  explore val = 0.951, episode reward = 12.0\n",
      "training episode 11 of 500 complete,  explore val = 0.946, episode reward = 38.0\n",
      "training episode 12 of 500 complete,  explore val = 0.942, episode reward = 22.0\n",
      "training episode 13 of 500 complete,  explore val = 0.937, episode reward = 37.0\n",
      "training episode 14 of 500 complete,  explore val = 0.932, episode reward = 15.0\n",
      "training episode 15 of 500 complete,  explore val = 0.928, episode reward = 13.0\n",
      "training episode 16 of 500 complete,  explore val = 0.923, episode reward = 41.0\n",
      "training episode 17 of 500 complete,  explore val = 0.918, episode reward = 13.0\n",
      "training episode 18 of 500 complete,  explore val = 0.914, episode reward = 66.0\n",
      "training episode 19 of 500 complete,  explore val = 0.909, episode reward = 18.0\n",
      "training episode 20 of 500 complete,  explore val = 0.905, episode reward = 42.0\n",
      "training episode 21 of 500 complete,  explore val = 0.9, episode reward = 45.0\n",
      "training episode 22 of 500 complete,  explore val = 0.896, episode reward = 22.0\n",
      "training episode 23 of 500 complete,  explore val = 0.891, episode reward = 10.0\n",
      "training episode 24 of 500 complete,  explore val = 0.887, episode reward = 14.0\n",
      "training episode 25 of 500 complete,  explore val = 0.882, episode reward = 43.0\n",
      "training episode 26 of 500 complete,  explore val = 0.878, episode reward = 15.0\n",
      "training episode 27 of 500 complete,  explore val = 0.873, episode reward = 18.0\n",
      "training episode 28 of 500 complete,  explore val = 0.869, episode reward = 26.0\n",
      "training episode 29 of 500 complete,  explore val = 0.865, episode reward = 9.0\n",
      "training episode 30 of 500 complete,  explore val = 0.86, episode reward = 27.0\n",
      "training episode 31 of 500 complete,  explore val = 0.856, episode reward = 12.0\n",
      "training episode 32 of 500 complete,  explore val = 0.852, episode reward = 120.0\n",
      "training episode 33 of 500 complete,  explore val = 0.848, episode reward = 13.0\n",
      "training episode 34 of 500 complete,  explore val = 0.843, episode reward = 41.0\n",
      "training episode 35 of 500 complete,  explore val = 0.839, episode reward = 11.0\n",
      "training episode 36 of 500 complete,  explore val = 0.835, episode reward = 18.0\n",
      "training episode 37 of 500 complete,  explore val = 0.831, episode reward = 32.0\n",
      "training episode 38 of 500 complete,  explore val = 0.827, episode reward = 16.0\n",
      "training episode 39 of 500 complete,  explore val = 0.822, episode reward = 18.0\n",
      "training episode 40 of 500 complete,  explore val = 0.818, episode reward = 23.0\n",
      "training episode 41 of 500 complete,  explore val = 0.814, episode reward = 13.0\n",
      "training episode 42 of 500 complete,  explore val = 0.81, episode reward = 16.0\n",
      "training episode 43 of 500 complete,  explore val = 0.806, episode reward = 21.0\n",
      "training episode 44 of 500 complete,  explore val = 0.802, episode reward = 13.0\n",
      "training episode 45 of 500 complete,  explore val = 0.798, episode reward = 72.0\n",
      "training episode 46 of 500 complete,  explore val = 0.794, episode reward = 18.0\n",
      "training episode 47 of 500 complete,  explore val = 0.79, episode reward = 43.0\n",
      "training episode 48 of 500 complete,  explore val = 0.786, episode reward = 21.0\n",
      "training episode 49 of 500 complete,  explore val = 0.782, episode reward = 53.0\n",
      "training episode 50 of 500 complete,  explore val = 0.778, episode reward = 23.0\n",
      "training episode 51 of 500 complete,  explore val = 0.774, episode reward = 63.0\n",
      "training episode 52 of 500 complete,  explore val = 0.771, episode reward = 46.0\n",
      "training episode 53 of 500 complete,  explore val = 0.767, episode reward = 16.0\n",
      "training episode 54 of 500 complete,  explore val = 0.763, episode reward = 24.0\n",
      "training episode 55 of 500 complete,  explore val = 0.759, episode reward = 16.0\n",
      "training episode 56 of 500 complete,  explore val = 0.755, episode reward = 46.0\n",
      "training episode 57 of 500 complete,  explore val = 0.751, episode reward = 60.0\n",
      "training episode 58 of 500 complete,  explore val = 0.748, episode reward = 37.0\n",
      "training episode 59 of 500 complete,  explore val = 0.744, episode reward = 31.0\n",
      "training episode 60 of 500 complete,  explore val = 0.74, episode reward = 22.0\n",
      "training episode 61 of 500 complete,  explore val = 0.737, episode reward = 46.0\n",
      "training episode 62 of 500 complete,  explore val = 0.733, episode reward = 29.0\n",
      "training episode 63 of 500 complete,  explore val = 0.729, episode reward = 48.0\n",
      "training episode 64 of 500 complete,  explore val = 0.726, episode reward = 54.0\n",
      "training episode 65 of 500 complete,  explore val = 0.722, episode reward = 12.0\n",
      "training episode 66 of 500 complete,  explore val = 0.718, episode reward = 70.0\n",
      "training episode 67 of 500 complete,  explore val = 0.715, episode reward = 14.0\n",
      "training episode 68 of 500 complete,  explore val = 0.711, episode reward = 87.0\n",
      "training episode 69 of 500 complete,  explore val = 0.708, episode reward = 16.0\n",
      "training episode 70 of 500 complete,  explore val = 0.704, episode reward = 20.0\n",
      "training episode 71 of 500 complete,  explore val = 0.701, episode reward = 142.0\n",
      "training episode 72 of 500 complete,  explore val = 0.697, episode reward = 73.0\n",
      "training episode 73 of 500 complete,  explore val = 0.694, episode reward = 156.0\n",
      "training episode 74 of 500 complete,  explore val = 0.69, episode reward = 28.0\n",
      "training episode 75 of 500 complete,  explore val = 0.687, episode reward = 59.0\n",
      "training episode 76 of 500 complete,  explore val = 0.683, episode reward = 34.0\n",
      "training episode 77 of 500 complete,  explore val = 0.68, episode reward = 78.0\n",
      "training episode 78 of 500 complete,  explore val = 0.676, episode reward = 26.0\n",
      "training episode 79 of 500 complete,  explore val = 0.673, episode reward = 20.0\n",
      "training episode 80 of 500 complete,  explore val = 0.67, episode reward = 44.0\n",
      "training episode 81 of 500 complete,  explore val = 0.666, episode reward = 14.0\n",
      "training episode 82 of 500 complete,  explore val = 0.663, episode reward = 36.0\n",
      "training episode 83 of 500 complete,  explore val = 0.66, episode reward = 102.0\n",
      "training episode 84 of 500 complete,  explore val = 0.656, episode reward = 204.0\n",
      "training episode 85 of 500 complete,  explore val = 0.653, episode reward = 91.0\n",
      "training episode 86 of 500 complete,  explore val = 0.65, episode reward = 59.0\n",
      "training episode 87 of 500 complete,  explore val = 0.647, episode reward = 31.0\n",
      "training episode 88 of 500 complete,  explore val = 0.643, episode reward = 129.0\n",
      "training episode 89 of 500 complete,  explore val = 0.64, episode reward = 39.0\n",
      "training episode 90 of 500 complete,  explore val = 0.637, episode reward = 54.0\n",
      "training episode 91 of 500 complete,  explore val = 0.634, episode reward = 33.0\n",
      "training episode 92 of 500 complete,  explore val = 0.631, episode reward = 16.0\n",
      "training episode 93 of 500 complete,  explore val = 0.627, episode reward = 14.0\n",
      "training episode 94 of 500 complete,  explore val = 0.624, episode reward = 29.0\n",
      "training episode 95 of 500 complete,  explore val = 0.621, episode reward = 44.0\n",
      "training episode 96 of 500 complete,  explore val = 0.618, episode reward = 40.0\n",
      "training episode 97 of 500 complete,  explore val = 0.615, episode reward = 41.0\n",
      "training episode 98 of 500 complete,  explore val = 0.612, episode reward = 51.0\n",
      "training episode 99 of 500 complete,  explore val = 0.609, episode reward = 67.0\n",
      "training episode 100 of 500 complete,  explore val = 0.606, episode reward = 66.0\n",
      "training episode 101 of 500 complete,  explore val = 0.603, episode reward = 85.0\n",
      "training episode 102 of 500 complete,  explore val = 0.6, episode reward = 29.0\n",
      "training episode 103 of 500 complete,  explore val = 0.597, episode reward = 155.0\n",
      "training episode 104 of 500 complete,  explore val = 0.594, episode reward = 14.0\n",
      "training episode 105 of 500 complete,  explore val = 0.591, episode reward = 68.0\n",
      "training episode 106 of 500 complete,  explore val = 0.588, episode reward = 45.0\n",
      "training episode 107 of 500 complete,  explore val = 0.585, episode reward = 13.0\n",
      "training episode 108 of 500 complete,  explore val = 0.582, episode reward = 98.0\n",
      "training episode 109 of 500 complete,  explore val = 0.579, episode reward = 33.0\n",
      "training episode 110 of 500 complete,  explore val = 0.576, episode reward = 17.0\n",
      "training episode 111 of 500 complete,  explore val = 0.573, episode reward = 175.0\n",
      "training episode 112 of 500 complete,  explore val = 0.57, episode reward = 21.0\n",
      "training episode 113 of 500 complete,  explore val = 0.568, episode reward = 127.0\n",
      "training episode 114 of 500 complete,  explore val = 0.565, episode reward = 198.0\n",
      "training episode 115 of 500 complete,  explore val = 0.562, episode reward = 54.0\n",
      "training episode 116 of 500 complete,  explore val = 0.559, episode reward = 117.0\n",
      "training episode 117 of 500 complete,  explore val = 0.556, episode reward = 26.0\n",
      "training episode 118 of 500 complete,  explore val = 0.554, episode reward = 26.0\n",
      "training episode 119 of 500 complete,  explore val = 0.551, episode reward = 76.0\n",
      "training episode 120 of 500 complete,  explore val = 0.548, episode reward = 55.0\n",
      "training episode 121 of 500 complete,  explore val = 0.545, episode reward = 58.0\n",
      "training episode 122 of 500 complete,  explore val = 0.543, episode reward = 23.0\n",
      "training episode 123 of 500 complete,  explore val = 0.54, episode reward = 69.0\n",
      "training episode 124 of 500 complete,  explore val = 0.537, episode reward = 119.0\n",
      "training episode 125 of 500 complete,  explore val = 0.534, episode reward = 118.0\n",
      "training episode 126 of 500 complete,  explore val = 0.532, episode reward = 93.0\n",
      "training episode 127 of 500 complete,  explore val = 0.529, episode reward = 94.0\n",
      "training episode 128 of 500 complete,  explore val = 0.526, episode reward = 117.0\n",
      "training episode 129 of 500 complete,  explore val = 0.524, episode reward = 295.0\n",
      "training episode 130 of 500 complete,  explore val = 0.521, episode reward = 99.0\n",
      "training episode 131 of 500 complete,  explore val = 0.519, episode reward = 178.0\n",
      "training episode 132 of 500 complete,  explore val = 0.516, episode reward = 111.0\n",
      "training episode 133 of 500 complete,  explore val = 0.513, episode reward = 158.0\n",
      "training episode 134 of 500 complete,  explore val = 0.511, episode reward = 91.0\n",
      "training episode 135 of 500 complete,  explore val = 0.508, episode reward = 72.0\n",
      "training episode 136 of 500 complete,  explore val = 0.506, episode reward = 191.0\n",
      "training episode 137 of 500 complete,  explore val = 0.503, episode reward = 139.0\n",
      "training episode 138 of 500 complete,  explore val = 0.501, episode reward = 67.0\n",
      "training episode 139 of 500 complete,  explore val = 0.498, episode reward = 25.0\n",
      "training episode 140 of 500 complete,  explore val = 0.496, episode reward = 268.0\n",
      "training episode 141 of 500 complete,  explore val = 0.493, episode reward = 18.0\n",
      "training episode 142 of 500 complete,  explore val = 0.491, episode reward = 76.0\n",
      "training episode 143 of 500 complete,  explore val = 0.488, episode reward = 70.0\n",
      "training episode 144 of 500 complete,  explore val = 0.486, episode reward = 182.0\n",
      "training episode 145 of 500 complete,  explore val = 0.483, episode reward = 154.0\n",
      "training episode 146 of 500 complete,  explore val = 0.481, episode reward = 108.0\n",
      "training episode 147 of 500 complete,  explore val = 0.479, episode reward = 25.0\n",
      "training episode 148 of 500 complete,  explore val = 0.476, episode reward = 112.0\n",
      "training episode 149 of 500 complete,  explore val = 0.474, episode reward = 41.0\n",
      "training episode 150 of 500 complete,  explore val = 0.471, episode reward = 82.0\n",
      "training episode 151 of 500 complete,  explore val = 0.469, episode reward = 111.0\n",
      "training episode 152 of 500 complete,  explore val = 0.467, episode reward = 99.0\n",
      "training episode 153 of 500 complete,  explore val = 0.464, episode reward = 57.0\n",
      "training episode 154 of 500 complete,  explore val = 0.462, episode reward = 66.0\n",
      "training episode 155 of 500 complete,  explore val = 0.46, episode reward = 77.0\n",
      "training episode 156 of 500 complete,  explore val = 0.458, episode reward = 346.0\n",
      "training episode 157 of 500 complete,  explore val = 0.455, episode reward = 201.0\n",
      "training episode 158 of 500 complete,  explore val = 0.453, episode reward = 158.0\n",
      "training episode 159 of 500 complete,  explore val = 0.451, episode reward = 156.0\n",
      "training episode 160 of 500 complete,  explore val = 0.448, episode reward = 55.0\n",
      "training episode 161 of 500 complete,  explore val = 0.446, episode reward = 339.0\n",
      "training episode 162 of 500 complete,  explore val = 0.444, episode reward = 209.0\n",
      "training episode 163 of 500 complete,  explore val = 0.442, episode reward = 169.0\n",
      "training episode 164 of 500 complete,  explore val = 0.44, episode reward = 166.0\n",
      "training episode 165 of 500 complete,  explore val = 0.437, episode reward = 138.0\n",
      "training episode 166 of 500 complete,  explore val = 0.435, episode reward = 221.0\n",
      "training episode 167 of 500 complete,  explore val = 0.433, episode reward = 347.0\n",
      "training episode 168 of 500 complete,  explore val = 0.431, episode reward = 381.0\n",
      "training episode 169 of 500 complete,  explore val = 0.429, episode reward = 315.0\n",
      "training episode 170 of 500 complete,  explore val = 0.427, episode reward = 47.0\n",
      "training episode 171 of 500 complete,  explore val = 0.424, episode reward = 305.0\n",
      "training episode 172 of 500 complete,  explore val = 0.422, episode reward = 42.0\n",
      "training episode 173 of 500 complete,  explore val = 0.42, episode reward = 245.0\n",
      "training episode 174 of 500 complete,  explore val = 0.418, episode reward = 358.0\n",
      "training episode 175 of 500 complete,  explore val = 0.416, episode reward = 319.0\n",
      "training episode 176 of 500 complete,  explore val = 0.414, episode reward = 340.0\n",
      "training episode 177 of 500 complete,  explore val = 0.412, episode reward = 175.0\n",
      "training episode 178 of 500 complete,  explore val = 0.41, episode reward = 127.0\n",
      "training episode 179 of 500 complete,  explore val = 0.408, episode reward = 52.0\n",
      "training episode 180 of 500 complete,  explore val = 0.406, episode reward = 191.0\n",
      "training episode 181 of 500 complete,  explore val = 0.404, episode reward = 128.0\n",
      "training episode 182 of 500 complete,  explore val = 0.402, episode reward = 16.0\n",
      "training episode 183 of 500 complete,  explore val = 0.4, episode reward = 479.0\n",
      "training episode 184 of 500 complete,  explore val = 0.398, episode reward = 330.0\n",
      "training episode 185 of 500 complete,  explore val = 0.396, episode reward = 25.0\n",
      "training episode 186 of 500 complete,  explore val = 0.394, episode reward = 329.0\n",
      "training episode 187 of 500 complete,  explore val = 0.392, episode reward = 252.0\n",
      "training episode 188 of 500 complete,  explore val = 0.39, episode reward = 348.0\n",
      "training episode 189 of 500 complete,  explore val = 0.388, episode reward = 323.0\n",
      "training episode 190 of 500 complete,  explore val = 0.386, episode reward = 500.0\n",
      "training episode 191 of 500 complete,  explore val = 0.384, episode reward = 347.0\n",
      "training episode 192 of 500 complete,  explore val = 0.382, episode reward = 500.0\n",
      "training episode 193 of 500 complete,  explore val = 0.38, episode reward = 372.0\n",
      "training episode 194 of 500 complete,  explore val = 0.378, episode reward = 180.0\n",
      "training episode 195 of 500 complete,  explore val = 0.376, episode reward = 500.0\n",
      "training episode 196 of 500 complete,  explore val = 0.374, episode reward = 500.0\n",
      "training episode 197 of 500 complete,  explore val = 0.373, episode reward = 500.0\n",
      "training episode 198 of 500 complete,  explore val = 0.371, episode reward = 336.0\n",
      "training episode 199 of 500 complete,  explore val = 0.369, episode reward = 500.0\n",
      "training episode 200 of 500 complete,  explore val = 0.367, episode reward = 227.0\n",
      "training episode 201 of 500 complete,  explore val = 0.365, episode reward = 384.0\n",
      "training episode 202 of 500 complete,  explore val = 0.363, episode reward = 131.0\n",
      "training episode 203 of 500 complete,  explore val = 0.361, episode reward = 500.0\n",
      "training episode 204 of 500 complete,  explore val = 0.36, episode reward = 500.0\n",
      "training episode 205 of 500 complete,  explore val = 0.358, episode reward = 500.0\n",
      "training episode 206 of 500 complete,  explore val = 0.356, episode reward = 376.0\n",
      "training episode 207 of 500 complete,  explore val = 0.354, episode reward = 500.0\n",
      "training episode 208 of 500 complete,  explore val = 0.353, episode reward = 359.0\n",
      "training episode 209 of 500 complete,  explore val = 0.351, episode reward = 331.0\n",
      "training episode 210 of 500 complete,  explore val = 0.349, episode reward = 367.0\n",
      "training episode 211 of 500 complete,  explore val = 0.347, episode reward = 484.0\n",
      "training episode 212 of 500 complete,  explore val = 0.346, episode reward = 197.0\n",
      "training episode 213 of 500 complete,  explore val = 0.344, episode reward = 437.0\n",
      "training episode 214 of 500 complete,  explore val = 0.342, episode reward = 228.0\n",
      "training episode 215 of 500 complete,  explore val = 0.34, episode reward = 500.0\n",
      "training episode 216 of 500 complete,  explore val = 0.339, episode reward = 372.0\n",
      "training episode 217 of 500 complete,  explore val = 0.337, episode reward = 326.0\n",
      "training episode 218 of 500 complete,  explore val = 0.335, episode reward = 306.0\n",
      "training episode 219 of 500 complete,  explore val = 0.334, episode reward = 268.0\n",
      "training episode 220 of 500 complete,  explore val = 0.332, episode reward = 142.0\n",
      "training episode 221 of 500 complete,  explore val = 0.33, episode reward = 358.0\n",
      "training episode 222 of 500 complete,  explore val = 0.329, episode reward = 387.0\n",
      "training episode 223 of 500 complete,  explore val = 0.327, episode reward = 379.0\n",
      "training episode 224 of 500 complete,  explore val = 0.325, episode reward = 118.0\n",
      "training episode 225 of 500 complete,  explore val = 0.324, episode reward = 311.0\n",
      "training episode 226 of 500 complete,  explore val = 0.322, episode reward = 447.0\n",
      "training episode 227 of 500 complete,  explore val = 0.321, episode reward = 500.0\n",
      "training episode 228 of 500 complete,  explore val = 0.319, episode reward = 426.0\n",
      "training episode 229 of 500 complete,  explore val = 0.317, episode reward = 500.0\n",
      "training episode 230 of 500 complete,  explore val = 0.316, episode reward = 410.0\n",
      "training episode 231 of 500 complete,  explore val = 0.314, episode reward = 456.0\n",
      "training episode 232 of 500 complete,  explore val = 0.313, episode reward = 339.0\n",
      "training episode 233 of 500 complete,  explore val = 0.311, episode reward = 323.0\n",
      "training episode 234 of 500 complete,  explore val = 0.309, episode reward = 378.0\n",
      "training episode 235 of 500 complete,  explore val = 0.308, episode reward = 303.0\n",
      "training episode 236 of 500 complete,  explore val = 0.306, episode reward = 453.0\n",
      "training episode 237 of 500 complete,  explore val = 0.305, episode reward = 459.0\n",
      "training episode 238 of 500 complete,  explore val = 0.303, episode reward = 249.0\n",
      "training episode 239 of 500 complete,  explore val = 0.302, episode reward = 500.0\n",
      "training episode 240 of 500 complete,  explore val = 0.3, episode reward = 500.0\n",
      "training episode 241 of 500 complete,  explore val = 0.299, episode reward = 500.0\n",
      "training episode 242 of 500 complete,  explore val = 0.297, episode reward = 500.0\n",
      "training episode 243 of 500 complete,  explore val = 0.296, episode reward = 243.0\n",
      "training episode 244 of 500 complete,  explore val = 0.294, episode reward = 500.0\n",
      "training episode 245 of 500 complete,  explore val = 0.293, episode reward = 500.0\n",
      "training episode 246 of 500 complete,  explore val = 0.291, episode reward = 328.0\n",
      "training episode 247 of 500 complete,  explore val = 0.29, episode reward = 249.0\n",
      "training episode 248 of 500 complete,  explore val = 0.288, episode reward = 316.0\n",
      "training episode 249 of 500 complete,  explore val = 0.287, episode reward = 183.0\n",
      "training episode 250 of 500 complete,  explore val = 0.286, episode reward = 253.0\n",
      "training episode 251 of 500 complete,  explore val = 0.284, episode reward = 202.0\n",
      "training episode 252 of 500 complete,  explore val = 0.283, episode reward = 184.0\n",
      "training episode 253 of 500 complete,  explore val = 0.281, episode reward = 177.0\n",
      "training episode 254 of 500 complete,  explore val = 0.28, episode reward = 213.0\n",
      "training episode 255 of 500 complete,  explore val = 0.279, episode reward = 257.0\n",
      "training episode 256 of 500 complete,  explore val = 0.277, episode reward = 464.0\n",
      "training episode 257 of 500 complete,  explore val = 0.276, episode reward = 488.0\n",
      "training episode 258 of 500 complete,  explore val = 0.274, episode reward = 473.0\n",
      "training episode 259 of 500 complete,  explore val = 0.273, episode reward = 500.0\n",
      "training episode 260 of 500 complete,  explore val = 0.272, episode reward = 500.0\n",
      "training episode 261 of 500 complete,  explore val = 0.27, episode reward = 500.0\n",
      "training episode 262 of 500 complete,  explore val = 0.269, episode reward = 482.0\n",
      "training episode 263 of 500 complete,  explore val = 0.268, episode reward = 500.0\n",
      "training episode 264 of 500 complete,  explore val = 0.266, episode reward = 500.0\n",
      "training episode 265 of 500 complete,  explore val = 0.265, episode reward = 500.0\n",
      "training episode 266 of 500 complete,  explore val = 0.264, episode reward = 500.0\n",
      "training episode 267 of 500 complete,  explore val = 0.262, episode reward = 500.0\n",
      "training episode 268 of 500 complete,  explore val = 0.261, episode reward = 199.0\n",
      "training episode 269 of 500 complete,  explore val = 0.26, episode reward = 500.0\n",
      "training episode 270 of 500 complete,  explore val = 0.258, episode reward = 167.0\n",
      "training episode 271 of 500 complete,  explore val = 0.257, episode reward = 194.0\n",
      "training episode 272 of 500 complete,  explore val = 0.256, episode reward = 146.0\n",
      "training episode 273 of 500 complete,  explore val = 0.255, episode reward = 197.0\n",
      "training episode 274 of 500 complete,  explore val = 0.253, episode reward = 228.0\n",
      "training episode 275 of 500 complete,  explore val = 0.252, episode reward = 228.0\n",
      "training episode 276 of 500 complete,  explore val = 0.251, episode reward = 275.0\n",
      "training episode 277 of 500 complete,  explore val = 0.249, episode reward = 353.0\n",
      "training episode 278 of 500 complete,  explore val = 0.248, episode reward = 328.0\n",
      "training episode 279 of 500 complete,  explore val = 0.247, episode reward = 500.0\n",
      "training episode 280 of 500 complete,  explore val = 0.246, episode reward = 377.0\n",
      "training episode 281 of 500 complete,  explore val = 0.245, episode reward = 341.0\n",
      "training episode 282 of 500 complete,  explore val = 0.243, episode reward = 367.0\n",
      "training episode 283 of 500 complete,  explore val = 0.242, episode reward = 460.0\n",
      "training episode 284 of 500 complete,  explore val = 0.241, episode reward = 337.0\n",
      "training episode 285 of 500 complete,  explore val = 0.24, episode reward = 318.0\n",
      "training episode 286 of 500 complete,  explore val = 0.238, episode reward = 339.0\n",
      "training episode 287 of 500 complete,  explore val = 0.237, episode reward = 310.0\n",
      "training episode 288 of 500 complete,  explore val = 0.236, episode reward = 316.0\n",
      "training episode 289 of 500 complete,  explore val = 0.235, episode reward = 253.0\n",
      "training episode 290 of 500 complete,  explore val = 0.234, episode reward = 241.0\n",
      "training episode 291 of 500 complete,  explore val = 0.233, episode reward = 45.0\n",
      "training episode 292 of 500 complete,  explore val = 0.231, episode reward = 500.0\n",
      "training episode 293 of 500 complete,  explore val = 0.23, episode reward = 70.0\n",
      "training episode 294 of 500 complete,  explore val = 0.229, episode reward = 462.0\n",
      "training episode 295 of 500 complete,  explore val = 0.228, episode reward = 273.0\n",
      "training episode 296 of 500 complete,  explore val = 0.227, episode reward = 287.0\n",
      "training episode 297 of 500 complete,  explore val = 0.226, episode reward = 500.0\n",
      "training episode 298 of 500 complete,  explore val = 0.225, episode reward = 434.0\n",
      "training episode 299 of 500 complete,  explore val = 0.223, episode reward = 369.0\n",
      "training episode 300 of 500 complete,  explore val = 0.222, episode reward = 434.0\n",
      "training episode 301 of 500 complete,  explore val = 0.221, episode reward = 500.0\n",
      "training episode 302 of 500 complete,  explore val = 0.22, episode reward = 500.0\n",
      "training episode 303 of 500 complete,  explore val = 0.219, episode reward = 334.0\n",
      "training episode 304 of 500 complete,  explore val = 0.218, episode reward = 317.0\n",
      "training episode 305 of 500 complete,  explore val = 0.217, episode reward = 346.0\n",
      "training episode 306 of 500 complete,  explore val = 0.216, episode reward = 313.0\n",
      "training episode 307 of 500 complete,  explore val = 0.215, episode reward = 500.0\n",
      "training episode 308 of 500 complete,  explore val = 0.214, episode reward = 375.0\n",
      "training episode 309 of 500 complete,  explore val = 0.212, episode reward = 374.0\n",
      "training episode 310 of 500 complete,  explore val = 0.211, episode reward = 500.0\n",
      "training episode 311 of 500 complete,  explore val = 0.21, episode reward = 459.0\n",
      "training episode 312 of 500 complete,  explore val = 0.209, episode reward = 428.0\n",
      "training episode 313 of 500 complete,  explore val = 0.208, episode reward = 373.0\n",
      "training episode 314 of 500 complete,  explore val = 0.207, episode reward = 500.0\n",
      "training episode 315 of 500 complete,  explore val = 0.206, episode reward = 500.0\n",
      "training episode 316 of 500 complete,  explore val = 0.205, episode reward = 500.0\n",
      "training episode 317 of 500 complete,  explore val = 0.204, episode reward = 500.0\n",
      "training episode 318 of 500 complete,  explore val = 0.203, episode reward = 500.0\n",
      "training episode 319 of 500 complete,  explore val = 0.202, episode reward = 500.0\n",
      "training episode 320 of 500 complete,  explore val = 0.201, episode reward = 500.0\n",
      "training episode 321 of 500 complete,  explore val = 0.2, episode reward = 500.0\n",
      "training episode 322 of 500 complete,  explore val = 0.199, episode reward = 492.0\n",
      "training episode 323 of 500 complete,  explore val = 0.198, episode reward = 364.0\n",
      "training episode 324 of 500 complete,  explore val = 0.197, episode reward = 498.0\n",
      "training episode 325 of 500 complete,  explore val = 0.196, episode reward = 500.0\n",
      "training episode 326 of 500 complete,  explore val = 0.195, episode reward = 500.0\n",
      "training episode 327 of 500 complete,  explore val = 0.194, episode reward = 500.0\n",
      "training episode 328 of 500 complete,  explore val = 0.193, episode reward = 500.0\n",
      "training episode 329 of 500 complete,  explore val = 0.192, episode reward = 500.0\n",
      "training episode 330 of 500 complete,  explore val = 0.191, episode reward = 500.0\n",
      "training episode 331 of 500 complete,  explore val = 0.19, episode reward = 363.0\n",
      "training episode 332 of 500 complete,  explore val = 0.189, episode reward = 377.0\n",
      "training episode 333 of 500 complete,  explore val = 0.188, episode reward = 500.0\n",
      "training episode 334 of 500 complete,  explore val = 0.187, episode reward = 404.0\n",
      "training episode 335 of 500 complete,  explore val = 0.187, episode reward = 500.0\n",
      "training episode 336 of 500 complete,  explore val = 0.186, episode reward = 342.0\n",
      "training episode 337 of 500 complete,  explore val = 0.185, episode reward = 292.0\n",
      "training episode 338 of 500 complete,  explore val = 0.184, episode reward = 500.0\n",
      "training episode 339 of 500 complete,  explore val = 0.183, episode reward = 500.0\n",
      "training episode 340 of 500 complete,  explore val = 0.182, episode reward = 500.0\n",
      "training episode 341 of 500 complete,  explore val = 0.181, episode reward = 500.0\n",
      "training episode 342 of 500 complete,  explore val = 0.18, episode reward = 500.0\n",
      "training episode 343 of 500 complete,  explore val = 0.179, episode reward = 390.0\n",
      "training episode 344 of 500 complete,  explore val = 0.178, episode reward = 500.0\n",
      "training episode 345 of 500 complete,  explore val = 0.177, episode reward = 500.0\n",
      "training episode 346 of 500 complete,  explore val = 0.177, episode reward = 424.0\n",
      "training episode 347 of 500 complete,  explore val = 0.176, episode reward = 500.0\n",
      "training episode 348 of 500 complete,  explore val = 0.175, episode reward = 500.0\n",
      "training episode 349 of 500 complete,  explore val = 0.174, episode reward = 311.0\n",
      "training episode 350 of 500 complete,  explore val = 0.173, episode reward = 500.0\n",
      "training episode 351 of 500 complete,  explore val = 0.172, episode reward = 500.0\n",
      "training episode 352 of 500 complete,  explore val = 0.171, episode reward = 275.0\n",
      "training episode 353 of 500 complete,  explore val = 0.17, episode reward = 500.0\n",
      "training episode 354 of 500 complete,  explore val = 0.17, episode reward = 500.0\n",
      "training episode 355 of 500 complete,  explore val = 0.169, episode reward = 421.0\n",
      "training episode 356 of 500 complete,  explore val = 0.168, episode reward = 500.0\n",
      "training episode 357 of 500 complete,  explore val = 0.167, episode reward = 414.0\n",
      "training episode 358 of 500 complete,  explore val = 0.166, episode reward = 255.0\n",
      "training episode 359 of 500 complete,  explore val = 0.165, episode reward = 500.0\n",
      "training episode 360 of 500 complete,  explore val = 0.165, episode reward = 500.0\n",
      "training episode 361 of 500 complete,  explore val = 0.164, episode reward = 500.0\n",
      "training episode 362 of 500 complete,  explore val = 0.163, episode reward = 497.0\n",
      "training episode 363 of 500 complete,  explore val = 0.162, episode reward = 500.0\n",
      "training episode 364 of 500 complete,  explore val = 0.161, episode reward = 430.0\n",
      "training episode 365 of 500 complete,  explore val = 0.16, episode reward = 500.0\n",
      "training episode 366 of 500 complete,  explore val = 0.16, episode reward = 500.0\n",
      "training episode 367 of 500 complete,  explore val = 0.159, episode reward = 292.0\n",
      "training episode 368 of 500 complete,  explore val = 0.158, episode reward = 219.0\n",
      "training episode 369 of 500 complete,  explore val = 0.157, episode reward = 500.0\n",
      "training episode 370 of 500 complete,  explore val = 0.157, episode reward = 500.0\n",
      "training episode 371 of 500 complete,  explore val = 0.156, episode reward = 500.0\n",
      "training episode 372 of 500 complete,  explore val = 0.155, episode reward = 283.0\n",
      "training episode 373 of 500 complete,  explore val = 0.154, episode reward = 332.0\n",
      "training episode 374 of 500 complete,  explore val = 0.153, episode reward = 500.0\n",
      "training episode 375 of 500 complete,  explore val = 0.153, episode reward = 500.0\n",
      "training episode 376 of 500 complete,  explore val = 0.152, episode reward = 500.0\n",
      "training episode 377 of 500 complete,  explore val = 0.151, episode reward = 197.0\n",
      "training episode 378 of 500 complete,  explore val = 0.15, episode reward = 402.0\n",
      "training episode 379 of 500 complete,  explore val = 0.15, episode reward = 500.0\n",
      "training episode 380 of 500 complete,  explore val = 0.149, episode reward = 500.0\n",
      "training episode 381 of 500 complete,  explore val = 0.148, episode reward = 500.0\n",
      "training episode 382 of 500 complete,  explore val = 0.147, episode reward = 500.0\n",
      "training episode 383 of 500 complete,  explore val = 0.147, episode reward = 500.0\n",
      "training episode 384 of 500 complete,  explore val = 0.146, episode reward = 500.0\n",
      "training episode 385 of 500 complete,  explore val = 0.145, episode reward = 500.0\n",
      "training episode 386 of 500 complete,  explore val = 0.144, episode reward = 500.0\n",
      "training episode 387 of 500 complete,  explore val = 0.144, episode reward = 421.0\n",
      "training episode 388 of 500 complete,  explore val = 0.143, episode reward = 500.0\n",
      "training episode 389 of 500 complete,  explore val = 0.142, episode reward = 500.0\n",
      "training episode 390 of 500 complete,  explore val = 0.142, episode reward = 500.0\n",
      "training episode 391 of 500 complete,  explore val = 0.141, episode reward = 500.0\n",
      "training episode 392 of 500 complete,  explore val = 0.14, episode reward = 500.0\n",
      "training episode 393 of 500 complete,  explore val = 0.139, episode reward = 496.0\n",
      "training episode 394 of 500 complete,  explore val = 0.139, episode reward = 316.0\n",
      "training episode 395 of 500 complete,  explore val = 0.138, episode reward = 500.0\n",
      "training episode 396 of 500 complete,  explore val = 0.137, episode reward = 500.0\n",
      "training episode 397 of 500 complete,  explore val = 0.137, episode reward = 500.0\n",
      "training episode 398 of 500 complete,  explore val = 0.136, episode reward = 500.0\n",
      "training episode 399 of 500 complete,  explore val = 0.135, episode reward = 500.0\n",
      "training episode 400 of 500 complete,  explore val = 0.135, episode reward = 500.0\n",
      "training episode 401 of 500 complete,  explore val = 0.134, episode reward = 500.0\n",
      "training episode 402 of 500 complete,  explore val = 0.133, episode reward = 439.0\n",
      "training episode 403 of 500 complete,  explore val = 0.133, episode reward = 500.0\n",
      "training episode 404 of 500 complete,  explore val = 0.132, episode reward = 500.0\n",
      "training episode 405 of 500 complete,  explore val = 0.131, episode reward = 500.0\n",
      "training episode 406 of 500 complete,  explore val = 0.131, episode reward = 500.0\n",
      "training episode 407 of 500 complete,  explore val = 0.13, episode reward = 500.0\n",
      "training episode 408 of 500 complete,  explore val = 0.129, episode reward = 500.0\n",
      "training episode 409 of 500 complete,  explore val = 0.129, episode reward = 500.0\n",
      "training episode 410 of 500 complete,  explore val = 0.128, episode reward = 500.0\n",
      "training episode 411 of 500 complete,  explore val = 0.127, episode reward = 500.0\n",
      "training episode 412 of 500 complete,  explore val = 0.127, episode reward = 500.0\n",
      "training episode 413 of 500 complete,  explore val = 0.126, episode reward = 307.0\n",
      "training episode 414 of 500 complete,  explore val = 0.126, episode reward = 500.0\n",
      "training episode 415 of 500 complete,  explore val = 0.125, episode reward = 500.0\n",
      "training episode 416 of 500 complete,  explore val = 0.124, episode reward = 500.0\n",
      "training episode 417 of 500 complete,  explore val = 0.124, episode reward = 430.0\n",
      "training episode 418 of 500 complete,  explore val = 0.123, episode reward = 500.0\n",
      "training episode 419 of 500 complete,  explore val = 0.122, episode reward = 500.0\n",
      "training episode 420 of 500 complete,  explore val = 0.122, episode reward = 324.0\n",
      "training episode 421 of 500 complete,  explore val = 0.121, episode reward = 500.0\n",
      "training episode 422 of 500 complete,  explore val = 0.121, episode reward = 500.0\n",
      "training episode 423 of 500 complete,  explore val = 0.12, episode reward = 500.0\n",
      "training episode 424 of 500 complete,  explore val = 0.119, episode reward = 500.0\n",
      "training episode 425 of 500 complete,  explore val = 0.119, episode reward = 500.0\n",
      "training episode 426 of 500 complete,  explore val = 0.118, episode reward = 500.0\n",
      "training episode 427 of 500 complete,  explore val = 0.118, episode reward = 500.0\n",
      "training episode 428 of 500 complete,  explore val = 0.117, episode reward = 360.0\n",
      "training episode 429 of 500 complete,  explore val = 0.116, episode reward = 500.0\n",
      "training episode 430 of 500 complete,  explore val = 0.116, episode reward = 500.0\n",
      "training episode 431 of 500 complete,  explore val = 0.115, episode reward = 500.0\n",
      "training episode 432 of 500 complete,  explore val = 0.115, episode reward = 448.0\n",
      "training episode 433 of 500 complete,  explore val = 0.114, episode reward = 500.0\n",
      "training episode 434 of 500 complete,  explore val = 0.114, episode reward = 500.0\n",
      "training episode 435 of 500 complete,  explore val = 0.113, episode reward = 374.0\n",
      "training episode 436 of 500 complete,  explore val = 0.112, episode reward = 472.0\n",
      "training episode 437 of 500 complete,  explore val = 0.112, episode reward = 500.0\n",
      "training episode 438 of 500 complete,  explore val = 0.111, episode reward = 500.0\n",
      "training episode 439 of 500 complete,  explore val = 0.111, episode reward = 500.0\n",
      "training episode 440 of 500 complete,  explore val = 0.11, episode reward = 287.0\n",
      "training episode 441 of 500 complete,  explore val = 0.11, episode reward = 500.0\n",
      "training episode 442 of 500 complete,  explore val = 0.109, episode reward = 500.0\n",
      "training episode 443 of 500 complete,  explore val = 0.109, episode reward = 500.0\n",
      "training episode 444 of 500 complete,  explore val = 0.108, episode reward = 500.0\n",
      "training episode 445 of 500 complete,  explore val = 0.107, episode reward = 500.0\n",
      "training episode 446 of 500 complete,  explore val = 0.107, episode reward = 500.0\n",
      "training episode 447 of 500 complete,  explore val = 0.106, episode reward = 500.0\n",
      "training episode 448 of 500 complete,  explore val = 0.106, episode reward = 500.0\n",
      "training episode 449 of 500 complete,  explore val = 0.105, episode reward = 500.0\n",
      "training episode 450 of 500 complete,  explore val = 0.105, episode reward = 500.0\n",
      "training episode 451 of 500 complete,  explore val = 0.104, episode reward = 500.0\n",
      "training episode 452 of 500 complete,  explore val = 0.104, episode reward = 500.0\n",
      "training episode 453 of 500 complete,  explore val = 0.103, episode reward = 500.0\n",
      "training episode 454 of 500 complete,  explore val = 0.103, episode reward = 500.0\n",
      "training episode 455 of 500 complete,  explore val = 0.102, episode reward = 500.0\n",
      "training episode 456 of 500 complete,  explore val = 0.102, episode reward = 500.0\n",
      "training episode 457 of 500 complete,  explore val = 0.101, episode reward = 500.0\n",
      "training episode 458 of 500 complete,  explore val = 0.101, episode reward = 500.0\n",
      "training episode 459 of 500 complete,  explore val = 0.1, episode reward = 488.0\n",
      "training episode 460 of 500 complete,  explore val = 0.1, episode reward = 500.0\n",
      "training episode 461 of 500 complete,  explore val = 0.099, episode reward = 500.0\n",
      "training episode 462 of 500 complete,  explore val = 0.099, episode reward = 500.0\n",
      "training episode 463 of 500 complete,  explore val = 0.098, episode reward = 500.0\n",
      "training episode 464 of 500 complete,  explore val = 0.098, episode reward = 500.0\n",
      "training episode 465 of 500 complete,  explore val = 0.097, episode reward = 500.0\n",
      "training episode 466 of 500 complete,  explore val = 0.097, episode reward = 500.0\n",
      "training episode 467 of 500 complete,  explore val = 0.096, episode reward = 500.0\n",
      "training episode 468 of 500 complete,  explore val = 0.096, episode reward = 500.0\n",
      "training episode 469 of 500 complete,  explore val = 0.095, episode reward = 500.0\n",
      "training episode 470 of 500 complete,  explore val = 0.095, episode reward = 500.0\n",
      "training episode 471 of 500 complete,  explore val = 0.094, episode reward = 500.0\n",
      "training episode 472 of 500 complete,  explore val = 0.094, episode reward = 500.0\n",
      "training episode 473 of 500 complete,  explore val = 0.093, episode reward = 500.0\n",
      "training episode 474 of 500 complete,  explore val = 0.093, episode reward = 500.0\n",
      "training episode 475 of 500 complete,  explore val = 0.092, episode reward = 500.0\n",
      "training episode 476 of 500 complete,  explore val = 0.092, episode reward = 500.0\n",
      "training episode 477 of 500 complete,  explore val = 0.092, episode reward = 500.0\n",
      "training episode 478 of 500 complete,  explore val = 0.091, episode reward = 450.0\n",
      "training episode 479 of 500 complete,  explore val = 0.091, episode reward = 500.0\n",
      "training episode 480 of 500 complete,  explore val = 0.09, episode reward = 500.0\n",
      "training episode 481 of 500 complete,  explore val = 0.09, episode reward = 500.0\n",
      "training episode 482 of 500 complete,  explore val = 0.089, episode reward = 500.0\n",
      "training episode 483 of 500 complete,  explore val = 0.089, episode reward = 370.0\n",
      "training episode 484 of 500 complete,  explore val = 0.088, episode reward = 500.0\n",
      "training episode 485 of 500 complete,  explore val = 0.088, episode reward = 500.0\n",
      "training episode 486 of 500 complete,  explore val = 0.088, episode reward = 500.0\n",
      "training episode 487 of 500 complete,  explore val = 0.087, episode reward = 500.0\n",
      "training episode 488 of 500 complete,  explore val = 0.087, episode reward = 500.0\n",
      "training episode 489 of 500 complete,  explore val = 0.086, episode reward = 500.0\n",
      "training episode 490 of 500 complete,  explore val = 0.086, episode reward = 500.0\n",
      "training episode 491 of 500 complete,  explore val = 0.085, episode reward = 500.0\n",
      "training episode 492 of 500 complete,  explore val = 0.085, episode reward = 494.0\n",
      "training episode 493 of 500 complete,  explore val = 0.084, episode reward = 500.0\n",
      "training episode 494 of 500 complete,  explore val = 0.084, episode reward = 500.0\n",
      "training episode 495 of 500 complete,  explore val = 0.084, episode reward = 500.0\n",
      "training episode 496 of 500 complete,  explore val = 0.083, episode reward = 500.0\n",
      "training episode 497 of 500 complete,  explore val = 0.083, episode reward = 500.0\n",
      "training episode 498 of 500 complete,  explore val = 0.082, episode reward = 500.0\n",
      "training episode 499 of 500 complete,  explore val = 0.082, episode reward = 500.0\n",
      "training episode 500 of 500 complete,  explore val = 0.082, episode reward = 500.0\n",
      "q-learning algorithm complete\n"
     ]
    }
   ],
   "source": [
    "demo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot total episode reward history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_reward_history(logname,**kwargs):\n",
    "    # load in total episode reward history\n",
    "    data = np.loadtxt(logname)\n",
    "    ave = [data[v] for v in range(100)]\n",
    "    \"\"\n",
    "    for i in range(0,np.size(data)-100):\n",
    "        m = np.mean(data[i:i+100])\n",
    "        ave.append(m)\n",
    "    \n",
    "    # create figure\n",
    "    fig = plt.figure(figsize = (12,8))\n",
    "    ax1 = fig.add_subplot(2,1,1)\n",
    "    ax2 = fig.add_subplot(2,1,2)\n",
    "\n",
    "    # plot total reward history\n",
    "    start = 0\n",
    "    if 'start' in kwargs:\n",
    "        start = kwargs['start']\n",
    "    ax1.plot(data[start:])\n",
    "    ax1.set_xlabel('episode',labelpad = 8,fontsize = 13)\n",
    "    ax1.set_ylabel('total reward',fontsize = 13)\n",
    "    \n",
    "    ax2.plot(ave[start:],linewidth=3)\n",
    "    ax2.set_xlabel('episode',labelpad = 8,fontsize = 13)\n",
    "    ax2.set_ylabel('ave total reward',fontsize=13)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reward_logname = 'reward_logs/' + savename + '.txt'\n",
    "plot_reward_history(reward_logname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
